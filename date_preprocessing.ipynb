{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3388a824",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627cb9b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import praw # Reddit API wrapper\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from io import StringIO\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import glob\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1100ca6",
   "metadata": {},
   "source": [
    "## S&P 500 Company Parser \n",
    "\n",
    "From Wikipedia : https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3da411",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "sp500 = pd.read_html(StringIO(str(table)))[0]\n",
    "sp500.head()\n",
    "sp500.to_csv(\"./data/sp500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f90fe1c",
   "metadata": {},
   "source": [
    "## Reddit Data Extractor API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23044157",
   "metadata": {},
   "source": [
    "First let's load the api environment to collect all the necessary data. \n",
    "The idea is to download posts and comments from different subreddits. \n",
    "- Subreddits: 'wallstreetbets', 'stocks', 'investing', 'StockMarket'\n",
    "- General Search & Stock Specific search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc577fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will load the environment\n",
    "load_dotenv('api.env')\n",
    "# Import the ids and secret keys from our environment\n",
    "id = os.getenv('REDDIT_CLIENT_ID')\n",
    "secret = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "agent = os.getenv('REDDIT_USER_AGENT', 'StockDataScraper v1.0')\n",
    "reddit = praw.Reddit(client_id = id,\n",
    "                     client_secret = secret,\n",
    "                     user_agent = agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec01892",
   "metadata": {},
   "source": [
    "### Defining the Function that collects and downloads the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57626112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_data(reddit, subreddit_name, data_type='posts', search_term=None, \n",
    "                   time_filter='year', limit=200, comment_limit=30):\n",
    "    \"\"\"This is the general function that I will loop through in order to download\n",
    "      all the Reddit Data\"\"\"\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_list = []\n",
    "    comments_list = []\n",
    "    \n",
    "    try:\n",
    "        # Determine which data to fetch\n",
    "        if data_type == 'search' and search_term:\n",
    "            print(f\"Searching for '{search_term}' in r/{subreddit_name}...\")\n",
    "            posts = subreddit.search(search_term, limit=limit)\n",
    "            search_keywords = [search_term.lower(), f\"${search_term.lower()}\"]\n",
    "        else:\n",
    "            print(f\"Getting top posts from r/{subreddit_name} for {time_filter}...\")\n",
    "            posts = subreddit.top(time_filter=time_filter, limit=limit)\n",
    "            search_keywords = None\n",
    "            \n",
    "        # Process posts\n",
    "        for i, post in enumerate(posts):\n",
    "            # Filter search results if needed\n",
    "            if search_keywords and not any(kw in (post.title + \" \" + post.selftext).lower() for kw in search_keywords):\n",
    "                continue\n",
    "                \n",
    "            # Extract post data\n",
    "            post_data = {\n",
    "                'post_id': post.id,\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'score': post.score,\n",
    "                'upvote_ratio': post.upvote_ratio,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc),\n",
    "                'num_comments': post.num_comments,\n",
    "                'author': str(post.author),\n",
    "                'permalink': post.permalink,\n",
    "                'url': post.url,\n",
    "                'is_self': post.is_self,\n",
    "                'flair': post.link_flair_text,\n",
    "                'subreddit': subreddit_name,\n",
    "                'category': 'stock_specific' if data_type == 'search' else 'general'\n",
    "            }\n",
    "            \n",
    "            # Add search term \n",
    "            if search_term:\n",
    "                post_data['search_term'] = search_term\n",
    "                \n",
    "            posts_list.append(post_data)\n",
    "            \n",
    "            # Get comments\n",
    "            try:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                for comment in post.comments.list()[:comment_limit]:\n",
    "                    # Filter comments for search terms \n",
    "                    if search_keywords and not any(kw in comment.body.lower() for kw in search_keywords):\n",
    "                        continue\n",
    "                        \n",
    "                    comment_data = {\n",
    "                        'comment_id': comment.id,\n",
    "                        'post_id': post.id,\n",
    "                        'parent_id': comment.parent_id,\n",
    "                        'body': comment.body,\n",
    "                        'score': comment.score,\n",
    "                        'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "                        'author': str(comment.author),\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'category': 'stock_specific' if data_type == 'search' else 'general'\n",
    "                    }\n",
    "                    \n",
    "                    # Add search\n",
    "                    if search_term:\n",
    "                        comment_data['search_term'] = search_term\n",
    "                        \n",
    "                    comments_list.append(comment_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing comments for post {post.id}: {e}\")\n",
    "                \n",
    "            # Be nice to Reddit's servers\n",
    "                \n",
    "        print(f\"Found {len(posts_list)} posts and {len(comments_list)} comments\")\n",
    "        return pd.DataFrame(posts_list) if posts_list else pd.DataFrame(), \\\n",
    "               pd.DataFrame(comments_list) if comments_list else pd.DataFrame()\n",
    "               \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from r/{subreddit_name}: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de094494",
   "metadata": {},
   "source": [
    "### First Download General posts from the Subreddits\n",
    "- Economics\n",
    "- Finance\n",
    "- Wall streetbets\n",
    "- stocks\n",
    "- investing\n",
    "\n",
    "Making sure to get enough posts and number of comments per post. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S&P 500 tickers\n",
    "sptickers = sp500['Symbol'].tolist()\n",
    "# List of finance subreddits\n",
    "subreddits = ['Economics','finance','wallstreetbets', 'stocks', 'investing']\n",
    "# Initialize aggregated DataFrames\n",
    "\n",
    "all_posts = pd.DataFrame()\n",
    "all_comments = pd.DataFrame()\n",
    "\n",
    "# Loop through all the subreddits defined above and collect the posts and comments data\n",
    "for subreddit in subreddits:\n",
    "    posts_df, comments_df = get_reddit_data(reddit, subreddit,data_type='posts',\n",
    "                                            limit=200,comment_limit=30)\n",
    "    all_posts = pd.concat([all_posts, posts_df],ignore_index=True)\n",
    "    all_comments = pd.concat([all_comments, comments_df],ignore_index=True)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Safe all posts and comments into our file\n",
    "all_posts.to_csv('./data/reddit/general_posts.csv',index = False)\n",
    "all_comments.to_csv('./data/reddit/general_comments.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9387b",
   "metadata": {},
   "source": [
    "### Use the same function this time for specific stock companies\n",
    "\n",
    "The function loops through different ticker values inside each subreddit to fetch data to have rich dataset (i.e. 500 searches for each subreddit for which there are 5) with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S&P 500 tickers\n",
    "sptickers = sp500['Symbol'].tolist()\n",
    "\n",
    "subreddits = ['stocks','wallstreetbets','investing','Economics','finance']\n",
    "all_posts = pd.DataFrame()\n",
    "all_comments = pd.DataFrame()\n",
    "for subreddit in subreddits:\n",
    "    for ticker in sptickers:\n",
    "        posts_df , comments_df = get_reddit_data(\n",
    "            reddit, subreddit, data_type='search',search_term=ticker,\n",
    "            limit=80, comment_limit=20\n",
    "        )\n",
    "        all_posts= pd.concat([all_posts, posts_df],ignore_index = True)\n",
    "        all_comments = pd.concat([all_comments,comments_df],ignore_index=True)\n",
    "        time.sleep(0.5)\n",
    "# Save the stock-specific posts and comments to CSV files\n",
    "all_posts.to_csv('./data/reddit/posts_stock_specific.csv')\n",
    "all_comments.to_csv('./data/reddit/comments_stock_specific.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113d10d",
   "metadata": {},
   "source": [
    "## Financial Data Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a17e4e",
   "metadata": {},
   "source": [
    "### First set up a file directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "data_dir = \"./data/financial_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d3f52",
   "metadata": {},
   "source": [
    "### Seting up SEC's EDGAR API to download\n",
    "\n",
    "\n",
    "You can find more information @ https://www.sec.gov/search-filings/edgar-application-programming-interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35201a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set my identity for the SEC (First name last name and email)\n",
    "name = \"Maseeh\"\n",
    "surname = \"Faizan\"\n",
    "email = \"maseehfaizan@unil.ch\"\n",
    "headers = {'User-Agent': f'{name} {surname} {email}'}\n",
    "\n",
    "# Get ticker data\n",
    "ticker = requests.get('https://www.sec.gov/files/company_tickers.json', headers=headers).json()\n",
    "ticker_df = pd.DataFrame.from_dict(ticker, orient='index')\n",
    "ticker_df.rename(columns={'cik_str':'cik','title':'name'}, inplace=True)\n",
    "\n",
    "# Fill in the cik code and add the leading zeros (Usually we need 10 digits with leading zeros)\n",
    "ticker_df['cik'] = ticker_df['cik'].astype(str).str.zfill(10)\n",
    "\n",
    "print(f\"Loaded ticker data for {len(ticker_df)} companies\")\n",
    "print(\"Sample ticker data:\")\n",
    "\n",
    "\n",
    "# API endpoints\n",
    "COMPANY_FACTS_URL = \"https://data.sec.gov/api/xbrl/companyfacts/CIK{}.json\"\n",
    "\n",
    "print(\"API configuration set!\")\n",
    "\n",
    "# Variables to track progress\n",
    "processed_companies = []\n",
    "failed_companies = []\n",
    "all_raw_data = []\n",
    "\n",
    "# Processing parameters\n",
    "delay_between_requests = 0.5  # seconds\n",
    "max_retries = 3\n",
    "\n",
    "print(\"Ready to start downloading!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29976a4",
   "metadata": {},
   "source": [
    "### Mapping function to map wikipedia data with SEC's JSON Date\n",
    "\n",
    "- I have fetched company ticker data from the SEC's EDGAR API but I also have the ticker data from the Wikipedia\n",
    "- The function bellow makes sure the ticker values are all UPPERCASE and can locate the cik value with leading zeros for the specific ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to get CIK from ticker using your ticker_df\n",
    "def get_cik_from_ticker(symbol):\n",
    "    \"\"\"Get CIK for a given ticker symbol from ticker_df\"\"\"\n",
    "    result = ticker_df[ticker_df['ticker'] == symbol.upper()]\n",
    "    if not result.empty:\n",
    "        return result.iloc[0]['cik']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07869e83",
   "metadata": {},
   "source": [
    "### Main download function\n",
    "\n",
    "This code downloads raw financial data for S&P 500 companies from the SEC EDGAR database. It iterates through each company, retrieves its CIK, and then fetches and processes US-GAAP financial facts. The extracted data for each company is saved as a separate CSV file and appended to a master list for comprehensive analysis.\n",
    "\n",
    "- This code will output **RAW** Financial data that still needs to be processed. It gives all of the accounts that are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4152e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_symbols = sp500['Symbol'].tolist()\n",
    "for i, symbol in enumerate(tqdm(company_symbols, desc=\"Downloading financial data\")):\n",
    "    print(f\"\\nProcessing {symbol} ({i+1}/{len(company_symbols)})\")\n",
    "    \n",
    "    # Get CIK for the symbol using your ticker_df\n",
    "    cik = get_cik_from_ticker(symbol)\n",
    "    \n",
    "    if not cik:\n",
    "        print(f\"Could not find CIK for {symbol}\")\n",
    "        failed_companies.append(symbol)\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found CIK: {cik} for {symbol}\")\n",
    "    \n",
    "    # Download company facts with retries\n",
    "    facts_data = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            url = COMPANY_FACTS_URL.format(cik)\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit hit, waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                facts_data = response.json()\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt+1}: Status code {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            time.sleep(delay_between_requests * (2 ** attempt))\n",
    "    \n",
    "    if not facts_data:\n",
    "        print(f\"Failed to get data for {symbol} after {max_retries} attempts\")\n",
    "        failed_companies.append(symbol)\n",
    "        continue\n",
    "    \n",
    "    # Extract raw financial data\n",
    "    if 'facts' in facts_data and 'us-gaap' in facts_data['facts']:\n",
    "        company_name = facts_data.get('entityName', 'Unknown')\n",
    "        print(f\"Processing data for {company_name}\")\n",
    "        \n",
    "        us_gaap_facts = facts_data['facts']['us-gaap']\n",
    "        \n",
    "        company_records = []\n",
    "        for concept, concept_data in us_gaap_facts.items():\n",
    "            units = concept_data.get('units', {})\n",
    "            \n",
    "            for unit_type, values in units.items():\n",
    "                for value_data in values:\n",
    "                    record = {\n",
    "                        'symbol': symbol,\n",
    "                        'cik': cik,\n",
    "                        'company_name': company_name,\n",
    "                        'concept': concept,\n",
    "                        'unit': unit_type,\n",
    "                        'start': value_data.get('start', None),\n",
    "                        'end': value_data.get('end', None),\n",
    "                        'filed': value_data.get('filed', None),\n",
    "                        'form': value_data.get('form', None),\n",
    "                        'frame': value_data.get('frame', None),\n",
    "                        'value': value_data.get('val', None)\n",
    "                    }\n",
    "                    company_records.append(record)\n",
    "        \n",
    "        if company_records:\n",
    "            # Convert to DataFrame and save individual company file\n",
    "            company_df = pd.DataFrame(company_records)\n",
    "            \n",
    "            # Convert dates\n",
    "            for date_col in ['start', 'end', 'filed']:\n",
    "                company_df[date_col] = pd.to_datetime(company_df[date_col], errors='coerce')\n",
    "            \n",
    "            # Save individual company raw data\n",
    "            company_file = os.path.join(data_dir, f\"{symbol}_raw_financials.csv\")\n",
    "            company_df.to_csv(company_file, index=False)\n",
    "            \n",
    "            # Add to master list\n",
    "            all_raw_data.extend(company_records)\n",
    "            processed_companies.append(symbol)\n",
    "            \n",
    "            print(f\"Saved {len(company_records)} records for {symbol}\")\n",
    "        else:\n",
    "            print(f\"No financial data found for {symbol}\")\n",
    "            failed_companies.append(symbol)\n",
    "    else:\n",
    "        print(f\"No US-GAAP facts found for {symbol}\")\n",
    "        failed_companies.append(symbol)\n",
    "    \n",
    "    # Wait between requests\n",
    "    time.sleep(delay_between_requests)\n",
    "\n",
    "print(f\"\\nDownload complete!\")\n",
    "print(f\"Successfully processed: {len(processed_companies)} companies\")\n",
    "print(f\"Failed: {len(failed_companies)} companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c1a2b",
   "metadata": {},
   "source": [
    "## 10-K file downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165674ed",
   "metadata": {},
   "source": [
    "The following code downloads 10-K annual reports for S&P 500 companies directly from the SEC EDGAR database. It iterates through each company, fetches its recent submissions, filters for 10-K filings, and then downloads each report as an HTML file, saving them to a local directory. It includes error handling and a small delay to be respectful of the SEC's servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for SEC API requests\n",
    "headers = {\n",
    "    'User-Agent': 'Maseeh Faizan maseeh.faizan@unil.ch'\n",
    "}\n",
    "\n",
    "# Base URL for SEC data\n",
    "base_url = \"https://data.sec.gov\"\n",
    "\n",
    "# Create directory for storing filings\n",
    "os.makedirs('./data/sec_filings', exist_ok=True)\n",
    "\n",
    "# Loop through all companies in the dataframe\n",
    "for _, row in sp500.iterrows():\n",
    "    company = row['Symbol']\n",
    "    cik = row['CIK']\n",
    "\n",
    "    print(f\"Processing {company} (CIK: {cik})...\")\n",
    "\n",
    "    try:\n",
    "        # Get company submissions\n",
    "        submissions_url = f\"{base_url}/submissions/CIK{cik:010d}.json\"\n",
    "        response = requests.get(submissions_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        submissions_df = pd.DataFrame(submissions['filings']['recent'])\n",
    "\n",
    "        # Filter for 10-K filings\n",
    "        submissions_df = submissions_df[submissions_df['form'] == '10-K']\n",
    "\n",
    "        # Create document URLs\n",
    "        submissions_df['doc_url'] = submissions_df.apply(\n",
    "            lambda x: f\"https://www.sec.gov/Archives/edgar/data/{cik}/{x['accessionNumber'].replace('-', '')}/{x['primaryDocument']}\",\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Skip if no 10-K filings found\n",
    "        if len(submissions_df) == 0:\n",
    "            print(f\"No 10-K filings found for {company}\")\n",
    "            continue\n",
    "\n",
    "        # Download each filing\n",
    "        for idx, filing_row in submissions_df.iterrows():\n",
    "            url = filing_row['doc_url']\n",
    "            form = filing_row['form']\n",
    "            date = filing_row['reportDate']\n",
    "\n",
    "            # Create filename with company symbol\n",
    "            filename = f\"{company}_{form}_{date}.html\"\n",
    "            file_path = os.path.join('sec_filings', filename)\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                print(f\"Downloaded: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {filename}: {e}\")\n",
    "\n",
    "        print(f\"Completed {company} ({len(submissions_df)} filings)\")\n",
    "\n",
    "        # Add a small delay to be respectful of SEC's servers\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company}: {e}\")\n",
    "\n",
    "print(\"All companies processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56662c",
   "metadata": {},
   "source": [
    "# Data Cleaning & Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e714be6",
   "metadata": {},
   "source": [
    "## Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b22997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading general and stock specific posts\n",
    "posts = pd.read_csv('./data/reddit/general_posts.csv').dropna(subset='category') ## Making sure to drop any empty cells\n",
    "stock_posts = pd.read_csv('./data/reddit/posts_stock_specific.csv').dropna(subset='category')\n",
    "\n",
    "# Loading comments and stock specific comments\n",
    "comments = pd.read_csv('./data/reddit/general_comments.csv')\n",
    "stock_comments = pd.read_csv('./data/reddit/comments_stock_specific.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e719b5",
   "metadata": {},
   "source": [
    "- First we will make sure the scores are considered as numbers and not strings\n",
    "- Group by each post and grab top 15 comments for each post based on the score \n",
    "- I want to keep all the comments together in the same cell so we will list them together\n",
    "- We will now merge the listed comments with the appropriate post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['score'] = pd.to_numeric(comments['score'])\n",
    "stock_comments['score'] = pd.to_numeric(stock_comments['score'])\n",
    "\n",
    "\n",
    "# Group by 'post_id', sort by 'score' within each group, and get the head (top 15)\n",
    "stock_comments = stock_comments.groupby('post_id').apply(lambda x: x.sort_values(by='score', ascending=False).head(15)).reset_index(drop=True)\n",
    "comments = comments.groupby('post_id').apply(lambda x: x.sort_values(by='score', ascending=False).head(15)).reset_index(drop=True)\n",
    "\n",
    "# Group comments by 'post_id' and aggregate the 'body' into lists\n",
    "stock_comments = stock_comments.groupby('post_id')['body'].apply(list).reset_index()\n",
    "comments = comments.groupby('post_id')['body'].apply(list).reset_index()\n",
    "\n",
    "# Merge posts with comments\n",
    "stocks = pd.merge(stock_posts, stock_comments, on='post_id', how='left')\n",
    "general = pd.merge(posts, comments, on='post_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6e005",
   "metadata": {},
   "source": [
    "### For the merged dataframe I will \n",
    "- Make sure the the post date are considered as proper pandas date\n",
    "- Rename the columns to have a better idea what the column is about \n",
    "- Finally concating and merging all the dataframe together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53757663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'created_utc' to datetime and keep only the date ---\n",
    "stocks['created_utc'] = pd.to_datetime(stocks['created_utc']).dt.date\n",
    "general['created_utc'] = pd.to_datetime(general['created_utc']).dt.date\n",
    "\n",
    "# Rename columns for consistency\n",
    "names = {'selftext':'post','body':'comments'}\n",
    "stocks = stocks.rename(columns=names)\n",
    "general = general.rename(columns=names)\n",
    "\n",
    "\n",
    "main_df = pd.concat([stocks,general], ignore_index=True)\n",
    "main_df.to_csv('./data/cleaned_stock.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a531a7",
   "metadata": {},
   "source": [
    "## Financial Data Cleaner/Processor\n",
    "\n",
    "- Import the Financial csv data\n",
    "- sort values by date\n",
    "- Pivot the table to make the concept appear as independant columns across different date&Time\n",
    "- Given that during the pivote there were a lot of duplicate that were generated, I group the columns by filed date and remove any NaN cells as to have a cleaned dataframe\n",
    "- To further clean the data, I removed the columns that are 95% empty. I cannot do any meaning full Time Series analysis on those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting data processing...\")\n",
    "sp = pd.read_csv('./data/sp500.csv')\n",
    "sp.head(3)\n",
    "for i in sp['Symbol']:\n",
    "    try:\n",
    "        print(f'Processing symbol: {i}')\n",
    "        file_path = f'./data/financial_data/{i}_raw_financials.csv'\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f'Successfully read data for {i}')\n",
    "\n",
    "        pivoted_df = df.pivot_table(\n",
    "            index=['cik', 'company_name', 'frame', 'unit', 'end', 'filed', 'form'],\n",
    "            columns='concept',\n",
    "            values='value'\n",
    "        ).reset_index()\n",
    "        print(f'Pivoted data for {i}')\n",
    "\n",
    "        metadata_cols = ['cik', 'company_name', 'form', 'frame', 'end', 'unit','filed']\n",
    "        consolidated_df = pd.DataFrame()\n",
    "        grouped = pivoted_df.groupby('end')\n",
    "\n",
    "        for end_date, group in grouped:\n",
    "            row_data = {'end': end_date}\n",
    "            for col in metadata_cols:\n",
    "                if col in group.columns:\n",
    "                    row_data[col] = group[col].iloc[0]\n",
    "            \n",
    "            for col in pivoted_df.columns:\n",
    "                if col not in metadata_cols and col != 'end':\n",
    "                    non_nan_values = group[col].dropna()\n",
    "                    if not non_nan_values.empty:\n",
    "                        row_data[col] = non_nan_values.iloc[0]\n",
    "                    else:\n",
    "                        row_data[col] = None\n",
    "            \n",
    "            consolidated_df = pd.concat([consolidated_df, pd.DataFrame([row_data])], ignore_index=True)\n",
    "\n",
    "        ##### I only want the yearly data not the quarterly data for now ####\n",
    "        consolidated_df = consolidated_df[consolidated_df['form'] == '10-K']\n",
    "        consolidated_df = consolidated_df[consolidated_df['frame'].astype(str).str.match((r'^CY\\d{4}$'))]\n",
    "        print(f'Filtered by frame format for {i}')\n",
    "\n",
    "\n",
    "        print(f'Filtering columns with >95% empty cells for {i}')\n",
    "        if not consolidated_df.empty:\n",
    "            nan_percentages = consolidated_df.isna().mean()\n",
    "            cleaned_df = consolidated_df.loc[:, nan_percentages < 0.95]\n",
    "            consolidated_df = cleaned_df\n",
    "            print(f'Removed sparse columns for {i}')\n",
    "        else:\n",
    "            print(f'Consolidated dataframe is empty for {i}, skipping NaN column removal.')\n",
    "\n",
    "        output_file_path = f'./data/clean/{i}.csv'\n",
    "        \n",
    "        # Ensure the directory exists before saving\n",
    "        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "        \n",
    "        consolidated_df.to_csv(output_file_path, index=False)\n",
    "        print(f'Successfully processed and saved data for {i} to {output_file_path}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found for symbol {i} at path: {file_path}. Skipping this symbol.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: File for symbol {i} is empty: {file_path}. Skipping this symbol.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: A required column is missing for symbol {i} (KeyError: {e}). Skipping this symbol.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing symbol {i}: {e}\")\n",
    "        print(f\"Skipping symbol {i} and continuing with the next one.\")\n",
    "\n",
    "print(\"Finished processing all symbols.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082f613",
   "metadata": {},
   "source": [
    "### Here I will further clean the data to construct the Balance Sheet, Income Statement and Cash Flow Statement for the Sector / Industry and save them into the csv files\n",
    "\n",
    "\n",
    "- Check each year (Frame) individually for each financial account\n",
    "- If the primary account has a NaN/empty value in a specific year\n",
    "- Look at the alternative account names for that same year\n",
    "- Use the alternative value when available\n",
    "\n",
    "### The important thing to note here is I needed a mapping code to translate XBRL accounting standards for different companies in different sectors and different sub-industry to work together. To do so I have creating different python files for each sector containing mapping dictionary for each account where each account has the main account, the alternative account and the childeren account\n",
    "\n",
    "This will output in Financial_statement folder all the Balance Sheet, Income Statement and Cashflow statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global list to store mapping debug information\n",
    "mapping_debug_log = []\n",
    "\n",
    "# Function to load industry mappings from sector-specific Python files\n",
    "# Function to load industry mappings from sector-specific Python files\n",
    "def load_industry_mapping(sector, sub_industry, mappings_dir='.'):\n",
    "    \"\"\"\n",
    "    Load the appropriate industry mapping based on sector and sub-industry.\n",
    "    \n",
    "    Args:\n",
    "        sector (str): GICS Sector \n",
    "        sub_industry (str): GICS Sub-Industry\n",
    "        mappings_dir (str): Directory containing the mapping Python files\n",
    "        \n",
    "    Returns:\n",
    "        dict: The mapping dictionary for the specified sector and sub-industry\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert sector name to filename format\n",
    "        sector_file = sector.lower().replace(' ', '').replace('&', '').replace('-', '') + '.py'\n",
    "        \n",
    "        # Check if file exists (using the specified directory)\n",
    "        sector_file_path = os.path.join(mappings_dir, sector_file)\n",
    "        if not os.path.exists(sector_file_path):\n",
    "            debug_msg = f\"Warning: Mapping file {sector_file_path} not found for {sector} - {sub_industry}.\"\n",
    "            print(debug_msg)\n",
    "            mapping_debug_log.append(debug_msg)\n",
    "            return None\n",
    "        \n",
    "        # Load the module dynamically\n",
    "        spec = importlib.util.spec_from_file_location(sector.lower(), sector_file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        \n",
    "        # Get the industry mappings\n",
    "        if hasattr(module, 'industry_mappings'):\n",
    "            industry_mappings = module.industry_mappings\n",
    "            \n",
    "            # Log available keys for debugging\n",
    "            available_keys = list(industry_mappings.keys())\n",
    "            debug_msg = f\"Processing {sector} - {sub_industry}... dict_keys({available_keys})\"\n",
    "            print(debug_msg)\n",
    "            mapping_debug_log.append(debug_msg)\n",
    "            \n",
    "            # Try multiple normalization strategies to find a match\n",
    "            \n",
    "            # Strategy 1: Direct key lookup without normalization\n",
    "            if sub_industry in industry_mappings:\n",
    "                debug_msg = f\"Found direct mapping for {sub_industry}\"\n",
    "                print(debug_msg)\n",
    "                mapping_debug_log.append(debug_msg)\n",
    "                return industry_mappings[sub_industry]\n",
    "            \n",
    "            # Generate different normalized versions of the sub_industry\n",
    "            normalized_versions = [\n",
    "                # Strategy 2: PascalCase with '&' → 'And'\n",
    "                ''.join(word.capitalize() for word in sub_industry.replace('&', ' And ').split()),\n",
    "                \n",
    "                # Strategy 3: PascalCase with '&' removed\n",
    "                ''.join(word.capitalize() for word in sub_industry.replace('&', ' ').split()),\n",
    "                \n",
    "                # Strategy 4: PascalCase with all punctuation removed\n",
    "                ''.join(word.capitalize() for word in ''.join(c if c.isalnum() or c.isspace() else ' ' for c in sub_industry).split()),\n",
    "            ]\n",
    "            \n",
    "            # Try each normalized version\n",
    "            for normalized in normalized_versions:\n",
    "                if normalized in industry_mappings:\n",
    "                    debug_msg = f\"Found mapping for {sub_industry} → {normalized}\"\n",
    "                    print(debug_msg)\n",
    "                    mapping_debug_log.append(debug_msg)\n",
    "                    return industry_mappings[normalized]\n",
    "            \n",
    "            # Try more aggressive partial matching approaches\n",
    "            # Create multiple normalized versions for comparison\n",
    "            sub_variants = [\n",
    "                # Remove spaces, convert to lowercase\n",
    "                sub_industry.lower().replace(' ', ''),\n",
    "                \n",
    "                # Remove spaces, '&', convert to lowercase\n",
    "                sub_industry.lower().replace(' ', '').replace('&', ''),\n",
    "                \n",
    "                # Replace '&' with 'and', remove spaces, convert to lowercase\n",
    "                sub_industry.lower().replace(' ', '').replace('&', 'and'),\n",
    "                \n",
    "                # Remove all non-alphanumeric chars, convert to lowercase\n",
    "                ''.join(c.lower() for c in sub_industry if c.isalnum())\n",
    "            ]\n",
    "            \n",
    "            # Try to match with each key using the variants\n",
    "            for key in industry_mappings:\n",
    "                # Create similar variants for the key\n",
    "                key_variants = [\n",
    "                    # Remove spaces, convert to lowercase\n",
    "                    key.lower().replace(' ', ''),\n",
    "                    \n",
    "                    # Remove spaces, 'And', convert to lowercase\n",
    "                    key.lower().replace(' ', '').replace('and', ''),\n",
    "                    \n",
    "                    # Remove all non-alphanumeric chars, convert to lowercase\n",
    "                    ''.join(c.lower() for c in key if c.isalnum())\n",
    "                ]\n",
    "                \n",
    "                # Check if any variant of sub_industry matches any variant of key\n",
    "                for sub_var in sub_variants:\n",
    "                    for key_var in key_variants:\n",
    "                        if sub_var == key_var or sub_var in key_var or key_var in sub_var:\n",
    "                            debug_msg = f\"Using mapping for {key} (variant match with {sub_industry})\"\n",
    "                            print(debug_msg)\n",
    "                            mapping_debug_log.append(debug_msg)\n",
    "                            return industry_mappings[key]\n",
    "                \n",
    "                # Special case for 'Accounts' suffix\n",
    "                if 'accounts' in key.lower():\n",
    "                    key_no_accounts = key.lower().replace('accounts', '')\n",
    "                    for sub_var in sub_variants:\n",
    "                        if sub_var in key_no_accounts or key_no_accounts in sub_var:\n",
    "                            debug_msg = f\"Using mapping for {key} (matched after removing 'Accounts')\"\n",
    "                            print(debug_msg)\n",
    "                            mapping_debug_log.append(debug_msg)\n",
    "                            return industry_mappings[key]\n",
    "            \n",
    "            # Use first industry mapping as fallback\n",
    "            default_key = next(iter(industry_mappings.keys()))\n",
    "            debug_msg = f\"No specific mapping found for {sub_industry}. Using default mapping for {sector}: {default_key}\"\n",
    "            print(debug_msg)\n",
    "            mapping_debug_log.append(debug_msg)\n",
    "            return industry_mappings[default_key]\n",
    "        else:\n",
    "            debug_msg = f\"No industry_mappings found in {sector_file} for {sector} - {sub_industry}.\"\n",
    "            print(debug_msg)\n",
    "            mapping_debug_log.append(debug_msg)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        debug_msg = f\"Error loading industry mapping for {sector} - {sub_industry}: {str(e)}\"\n",
    "        print(debug_msg)\n",
    "        mapping_debug_log.append(debug_msg)\n",
    "        return None\n",
    "\n",
    "# Load company data from CSV\n",
    "def load_company_data(ticker, data_dir='clean'):\n",
    "    \"\"\"\n",
    "    Load financial data for a specific company.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): Company ticker symbol\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Financial data for the company\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filepath = os.path.join(data_dir, f\"{ticker}.csv\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to extract account value using mapping info\n",
    "# Updated helper function to extract account value using mapping info\n",
    "def extract_account_value(df, account_info, available_columns):\n",
    "    \"\"\"\n",
    "    Extract account value from financial data using mapping information.\n",
    "    Checks cell by cell, using alternatives if primary value is NaN.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Company financial data\n",
    "        account_info (dict): Account mapping information\n",
    "        available_columns (set): Available columns in the DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Series: Account value from primary or alternatives\n",
    "    \"\"\"\n",
    "    # Initialize result series with NaN values\n",
    "    result = pd.Series(np.nan, index=df.index)\n",
    "    \n",
    "    # Iterate through each row (each reporting period)\n",
    "    for idx in df.index:\n",
    "        # Check primary tag first\n",
    "        if 'primary' in account_info and account_info['primary'] in available_columns:\n",
    "            primary_value = df.at[idx, account_info['primary']]\n",
    "            \n",
    "            # If primary value exists and is not NaN, use it\n",
    "            if pd.notna(primary_value):\n",
    "                result.at[idx] = primary_value\n",
    "                continue\n",
    "        \n",
    "        # If primary is NaN or missing, check alternatives\n",
    "        if 'alternatives' in account_info:\n",
    "            for alt in account_info['alternatives']:\n",
    "                if alt in available_columns:\n",
    "                    alt_value = df.at[idx, alt]\n",
    "                    if pd.notna(alt_value):\n",
    "                        result.at[idx] = alt_value\n",
    "                        break  # Use first non-NaN alternative\n",
    "        \n",
    "        # If still NaN, try children as a sum if available\n",
    "        if pd.isna(result.at[idx]) and 'children' in account_info and account_info['children']:\n",
    "            child_values = []\n",
    "            for child in account_info['children']:\n",
    "                if child in available_columns and pd.notna(df.at[idx, child]):\n",
    "                    child_values.append(df.at[idx, child])\n",
    "            \n",
    "            # If we found any child values, sum them\n",
    "            if child_values:\n",
    "                result.at[idx] = sum(child_values)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to reconstruct balance sheet\n",
    "def reconstruct_balance_sheet(df, industry_mapping):\n",
    "    \"\"\"\n",
    "    Reconstruct balance sheet using industry-specific mappings.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Company financial data\n",
    "        industry_mapping (dict): Industry-specific mappings\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Reconstructed balance sheet\n",
    "    \"\"\"\n",
    "    # Create new DataFrame for balance sheet\n",
    "    balance_sheet = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Copy metadata columns\n",
    "    id_columns = ['filed', 'company_name', 'end', 'unit', 'form', 'frame', 'cik']\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            balance_sheet[col] = df[col]\n",
    "    \n",
    "    # Get available columns\n",
    "    available_columns = set(df.columns)\n",
    "    \n",
    "    # Process balance sheet sections dynamically\n",
    "    balance_sheet_sections = ['Assets', 'Liabilities', 'Equity']\n",
    "    \n",
    "    for section in balance_sheet_sections:\n",
    "        if section in industry_mapping:\n",
    "            # Iterate through all sub-accounts in this section as defined in the mapping\n",
    "            for account_name, account_info in industry_mapping[section].items():\n",
    "                value = extract_account_value(df, account_info, available_columns)\n",
    "                balance_sheet[f\"{section} - {account_name}\"] = value\n",
    "    \n",
    "    # Add missing totals and validate\n",
    "    balance_sheet = add_missing_balance_sheet_totals(balance_sheet)\n",
    "    \n",
    "    # Remove columns with only NaN values\n",
    "    balance_sheet = remove_nan_only_columns(balance_sheet)\n",
    "    \n",
    "    return balance_sheet\n",
    "\n",
    "# Function to reconstruct income statement\n",
    "def reconstruct_income_statement(df, industry_mapping):\n",
    "    \"\"\"\n",
    "    Reconstruct income statement using industry-specific mappings.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Company financial data\n",
    "        industry_mapping (dict): Industry-specific mappings\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Reconstructed income statement\n",
    "    \"\"\"\n",
    "    # Create new DataFrame for income statement\n",
    "    income_statement = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Copy metadata columns\n",
    "    id_columns = ['filed', 'company_name', 'end', 'unit', 'form', 'frame', 'cik']\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            income_statement[col] = df[col]\n",
    "    \n",
    "    # Get available columns\n",
    "    available_columns = set(df.columns)\n",
    "    \n",
    "    # Process Income Statement dynamically\n",
    "    if 'IncomeStatement' in industry_mapping:\n",
    "        income_mapping = industry_mapping['IncomeStatement']\n",
    "        \n",
    "        # Iterate through all sections in the income statement mapping\n",
    "        for section_name, section_data in income_mapping.items():\n",
    "            # Check if this is a nested structure or direct account mapping\n",
    "            if isinstance(section_data, dict) and 'primary' in section_data:\n",
    "                # Direct account mapping (unnested)\n",
    "                value = extract_account_value(df, section_data, available_columns)\n",
    "                income_statement[f\"IncomeStatement - {section_name}\"] = value\n",
    "            else:\n",
    "                # Nested structure - process each account in the section\n",
    "                for account_name, account_info in section_data.items():\n",
    "                    # Check if this is a further nested structure\n",
    "                    if isinstance(account_info, dict) and 'primary' in account_info:\n",
    "                        # Direct account mapping\n",
    "                        value = extract_account_value(df, account_info, available_columns)\n",
    "                        income_statement[f\"IncomeStatement - {section_name} - {account_name}\"] = value\n",
    "                    else:\n",
    "                        # Further nested structure\n",
    "                        for sub_account_name, sub_account_info in account_info.items():\n",
    "                            value = extract_account_value(df, sub_account_info, available_columns)\n",
    "                            income_statement[f\"IncomeStatement - {section_name} - {account_name} - {sub_account_name}\"] = value\n",
    "    \n",
    "    # Remove columns with only NaN values\n",
    "    income_statement = remove_nan_only_columns(income_statement)\n",
    "    \n",
    "    return income_statement\n",
    "\n",
    "# Function to reconstruct cash flow statement\n",
    "def reconstruct_cash_flow_statement(df, industry_mapping):\n",
    "    \"\"\"\n",
    "    Reconstruct cash flow statement using industry-specific mappings.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Company financial data\n",
    "        industry_mapping (dict): Industry-specific mappings\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Reconstructed cash flow statement\n",
    "    \"\"\"\n",
    "    # Create new DataFrame for cash flow statement\n",
    "    cash_flow = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Copy metadata columns\n",
    "    id_columns = ['filed', 'company_name', 'end', 'unit', 'form', 'frame', 'cik']\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            cash_flow[col] = df[col]\n",
    "    \n",
    "    # Get available columns\n",
    "    available_columns = set(df.columns)\n",
    "    \n",
    "    # Process Cash Flow Statement dynamically\n",
    "    if 'CashFlowStatement' in industry_mapping:\n",
    "        cf_mapping = industry_mapping['CashFlowStatement']\n",
    "        \n",
    "        # Iterate through all sections in the cash flow mapping\n",
    "        for section_name, section_data in cf_mapping.items():\n",
    "            # Process each account in the section\n",
    "            for account_name, account_info in section_data.items():\n",
    "                # Check if this is a nested structure\n",
    "                if isinstance(account_info, dict) and 'primary' in account_info:\n",
    "                    # Direct account mapping\n",
    "                    value = extract_account_value(df, account_info, available_columns)\n",
    "                    cash_flow[f\"CashFlow - {section_name} - {account_name}\"] = value\n",
    "                else:\n",
    "                    # Nested structure\n",
    "                    for sub_account_name, sub_account_info in account_info.items():\n",
    "                        value = extract_account_value(df, sub_account_info, available_columns)\n",
    "                        cash_flow[f\"CashFlow - {section_name} - {account_name} - {sub_account_name}\"] = value\n",
    "    \n",
    "    # Remove columns with only NaN values\n",
    "    cash_flow = remove_nan_only_columns(cash_flow)\n",
    "    \n",
    "    return cash_flow\n",
    "\n",
    "# Function to add missing balance sheet totals (including your validation logic)\n",
    "def add_missing_balance_sheet_totals(balance_sheet):\n",
    "    \"\"\"\n",
    "    Adds missing total columns according to accounting relationships.\n",
    "    \n",
    "    Args:\n",
    "        balance_sheet: DataFrame with reconstructed balance sheet\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with missing totals computed where possible and validation columns\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result = balance_sheet.copy()\n",
    "    \n",
    "    # Define key total column names\n",
    "    total_assets_col = 'Assets - Total Assets'\n",
    "    total_liabilities_col = 'Liabilities - TotalLiabilities'\n",
    "    total_equity_col = 'Equity - TotalStockholdersEquity'\n",
    "    total_liab_equity_col = 'Equity - TotalLiabilitiesandEquity'\n",
    "    \n",
    "    # Ensure total columns exist before attempting calculations row-wise\n",
    "    for col in [total_assets_col, total_liabilities_col, total_equity_col, total_liab_equity_col]:\n",
    "        if col not in result.columns:\n",
    "            result[col] = pd.NA\n",
    "    \n",
    "    # Process row by row to handle NaN values in specific cells\n",
    "    for idx, row in result.iterrows():\n",
    "        # Case 1: Compute missing Total Liabilities\n",
    "        if (pd.notna(row[total_liab_equity_col]) and\n",
    "            pd.notna(row[total_equity_col]) and\n",
    "            pd.isna(row[total_liabilities_col])):\n",
    "            result.at[idx, total_liabilities_col] = (row[total_liab_equity_col] -\n",
    "                                                    row[total_equity_col])\n",
    "        \n",
    "        # Case 2: Compute missing Total Stockholders Equity\n",
    "        if (pd.notna(row[total_liab_equity_col]) and\n",
    "            pd.notna(row[total_liabilities_col]) and\n",
    "            pd.isna(row[total_equity_col])):\n",
    "            result.at[idx, total_equity_col] = (row[total_liab_equity_col] -\n",
    "                                              row[total_liabilities_col])\n",
    "        \n",
    "        # Case 3: Compute missing Total Liabilities and Equity\n",
    "        if pd.isna(row[total_liab_equity_col]):\n",
    "            if pd.notna(row[total_assets_col]):\n",
    "                # Set Total Liabilities and Equity = Total Assets (accounting equality)\n",
    "                result.at[idx, total_liab_equity_col] = row[total_assets_col]\n",
    "            elif (pd.notna(row[total_liabilities_col]) and\n",
    "                  pd.notna(row[total_equity_col])):\n",
    "                # Compute Total Liabilities and Equity as sum of components\n",
    "                result.at[idx, total_liab_equity_col] = (row[total_liabilities_col] +\n",
    "                                                       row[total_equity_col])\n",
    "    \n",
    "    # Add validation columns\n",
    "    result['Validation - A = L+E Difference'] = (result[total_assets_col] -\n",
    "                                               result[total_liab_equity_col])\n",
    "    \n",
    "    # This check validates if the sum of Liabilities and Equity components equals Total Liabilities and Equity\n",
    "    if total_liabilities_col in result.columns and total_equity_col in result.columns:\n",
    "        result['Validation - L+E Components Sum Difference'] = (result[total_liabilities_col] +\n",
    "                                                              result[total_equity_col] -\n",
    "                                                              result[total_liab_equity_col])\n",
    "    else:\n",
    "        result['Validation - L+E Components Sum Difference'] = pd.NA\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to remove columns that only contain NaN values\n",
    "def remove_nan_only_columns(df):\n",
    "    \"\"\"\n",
    "    Removes columns that contain only NaN values.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with NaN-only columns removed\n",
    "    \"\"\"\n",
    "    nan_cols = df.columns[df.isnull().all()].tolist()\n",
    "    return df.drop(columns=nan_cols)\n",
    "\n",
    "# Function to process SP500 companies with debug logging\n",
    "def process_sp500_companies(sp500_df, data_dir='clean', mappings_dir='.', output_dir='output'):\n",
    "    \"\"\"\n",
    "    Process all S&P 500 companies to generate financial statements.\n",
    "    \n",
    "    Args:\n",
    "        sp500_df (DataFrame): S&P 500 companies data\n",
    "        data_dir (str): Directory containing company CSV files\n",
    "        mappings_dir (str): Directory containing mapping Python files\n",
    "        output_dir (str): Directory to save output files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Debug information including mapping issues\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Track success and failures\n",
    "    results = {\n",
    "        'processed_successfully': [],\n",
    "        'failed_processing': [],\n",
    "        'mapping_issues': []\n",
    "    }\n",
    "    \n",
    "    # Process each company\n",
    "    for idx, row in sp500_df.iterrows():\n",
    "        ticker = row['Symbol']\n",
    "        sector = row['GICS Sector']\n",
    "        sub_industry = row['GICS Sub-Industry']\n",
    "        \n",
    "        # Clear previous entries in the global debug log for this company\n",
    "        global mapping_debug_log\n",
    "        mapping_debug_log = []\n",
    "        \n",
    "        print(f\"\\nProcessing {ticker} ({sector} - {sub_industry})...\")\n",
    "        \n",
    "        # Load company data with specified directory\n",
    "        df = load_company_data(ticker, data_dir=data_dir)\n",
    "        if df is None:\n",
    "            debug_msg = f\"Skipping {ticker} - Could not load data.\"\n",
    "            print(debug_msg)\n",
    "            results['failed_processing'].append({\n",
    "                'ticker': ticker,\n",
    "                'sector': sector,\n",
    "                'sub_industry': sub_industry,\n",
    "                'reason': 'Data load failure'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Load industry mapping with specified directory\n",
    "        industry_mapping = load_industry_mapping(sector, sub_industry, mappings_dir=mappings_dir)\n",
    "        if industry_mapping is None:\n",
    "            debug_msg = f\"Skipping {ticker} - Could not load industry mapping.\"\n",
    "            print(debug_msg)\n",
    "            results['failed_processing'].append({\n",
    "                'ticker': ticker,\n",
    "                'sector': sector,\n",
    "                'sub_industry': sub_industry,\n",
    "                'reason': 'Mapping load failure'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Generate balance sheet\n",
    "            balance_sheet = reconstruct_balance_sheet(df, industry_mapping)\n",
    "            balance_sheet_path = os.path.join(output_dir, f\"{ticker}_balance_sheet.csv\")\n",
    "            balance_sheet.to_csv(balance_sheet_path, index=False)\n",
    "            \n",
    "            # Generate income statement\n",
    "            income_statement = reconstruct_income_statement(df, industry_mapping)\n",
    "            income_statement_path = os.path.join(output_dir, f\"{ticker}_income_statement.csv\")\n",
    "            income_statement.to_csv(income_statement_path, index=False)\n",
    "            \n",
    "            # Generate cash flow statement\n",
    "            cash_flow = reconstruct_cash_flow_statement(df, industry_mapping)\n",
    "            cash_flow_path = os.path.join(output_dir, f\"{ticker}_cash_flow.csv\")\n",
    "            cash_flow.to_csv(cash_flow_path, index=False)\n",
    "            \n",
    "            success_msg = f\"Successfully generated financial statements for {ticker}\"\n",
    "            print(success_msg)\n",
    "            \n",
    "            # Record success\n",
    "            results['processed_successfully'].append({\n",
    "                'ticker': ticker,\n",
    "                'sector': sector,\n",
    "                'sub_industry': sub_industry\n",
    "            })\n",
    "            \n",
    "            # If we used default mapping, record this as a mapping issue\n",
    "            if any(\"No specific mapping found\" in log for log in mapping_debug_log):\n",
    "                results['mapping_issues'].append({\n",
    "                    'ticker': ticker,\n",
    "                    'sector': sector,\n",
    "                    'sub_industry': sub_industry,\n",
    "                    'debug_logs': mapping_debug_log.copy()\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing {ticker}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            results['failed_processing'].append({\n",
    "                'ticker': ticker,\n",
    "                'sector': sector,\n",
    "                'sub_industry': sub_industry,\n",
    "                'reason': str(e)\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    # Save the debug results to a JSON file\n",
    "    debug_file_path = os.path.join(output_dir, \"mapping_debug_results.json\")\n",
    "    import json\n",
    "    with open(debug_file_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDebug information saved to: {debug_file_path}\")\n",
    "    return results\n",
    "\n",
    "# Function to extract and show mapping issues\n",
    "def show_mapping_issues(results=None):\n",
    "    \"\"\"\n",
    "    Display mapping issues from processing results.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from process_sp500_companies\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with mapping issues\n",
    "    \"\"\"\n",
    "    if results is None:\n",
    "        # Try to load results from the default location\n",
    "        import json\n",
    "        try:\n",
    "            with open(\"output/mapping_debug_results.json\", 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except:\n",
    "            print(\"No results file found. Run process_sp500_companies first.\")\n",
    "            return None\n",
    "    \n",
    "    # Extract mapping issues\n",
    "    issues_data = []\n",
    "    for issue in results['mapping_issues']:\n",
    "        for log in issue['debug_logs']:\n",
    "            if \"No specific mapping found\" in log:\n",
    "                issues_data.append({\n",
    "                    'ticker': issue['ticker'],\n",
    "                    'sector': issue['sector'],\n",
    "                    'sub_industry': issue['sub_industry'],\n",
    "                    'log_message': log\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if issues_data:\n",
    "        import pandas as pd\n",
    "        issues_df = pd.DataFrame(issues_data)\n",
    "        return issues_df\n",
    "    else:\n",
    "        print(\"No mapping issues found.\")\n",
    "        return None\n",
    "\n",
    "# Display formatted balance sheet (optional for visualization)\n",
    "def display_balance_sheet(balance_sheet, in_billions=True):\n",
    "    \"\"\"\n",
    "    Display balance sheet in a readable format.\n",
    "    \n",
    "    Args:\n",
    "        balance_sheet: DataFrame with balance sheet data\n",
    "        in_billions: If True, display in billions; otherwise in millions\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with formatted balance sheet\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    formatted_bs = balance_sheet.copy()\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_cols = [col for col in formatted_bs.columns \n",
    "                   if any(col.startswith(prefix) for prefix in ['Assets', 'Liabilities', 'Equity', 'Validation'])]\n",
    "    \n",
    "    # Convert to billions or millions\n",
    "    divisor = 1_000_000_000 if in_billions else 1_000_000\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # Check if the column is numeric before dividing\n",
    "        if pd.api.types.is_numeric_dtype(formatted_bs[col]):\n",
    "            formatted_bs[col] = formatted_bs[col] / divisor\n",
    "    \n",
    "    # Format the date column if it exists\n",
    "    if 'end' in formatted_bs.columns:\n",
    "        try:\n",
    "            formatted_bs['end'] = pd.to_datetime(formatted_bs['end']).dt.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            pass  # Keep original format if conversion fails\n",
    "    \n",
    "    # Organize columns by section\n",
    "    metadata_cols = ['filed', 'company_name', 'end', 'unit', 'form', 'frame', 'cik']\n",
    "    asset_cols = [col for col in formatted_bs.columns if col.startswith('Assets')]\n",
    "    liability_cols = [col for col in formatted_bs.columns if col.startswith('Liabilities')]\n",
    "    equity_cols = [col for col in formatted_bs.columns if col.startswith('Equity')]\n",
    "    validation_cols = [col for col in formatted_bs.columns if col.startswith('Validation')]\n",
    "    \n",
    "    # Create ordered list of columns\n",
    "    ordered_cols = (\n",
    "        [col for col in metadata_cols if col in formatted_bs.columns] +\n",
    "        sorted([col for col in asset_cols if 'Total' not in col]) +\n",
    "        sorted([col for col in asset_cols if 'Total' in col]) +\n",
    "        sorted([col for col in liability_cols if 'Total' not in col]) +\n",
    "        sorted([col for col in liability_cols if 'Total' in col]) +\n",
    "        sorted([col for col in equity_cols if 'Total' not in col and 'Liabilities and Equity' not in col]) +\n",
    "        sorted([col for col in equity_cols if 'Total Stockholders Equity' in col]) +\n",
    "        sorted([col for col in equity_cols if 'Liabilities and Equity' in col]) +\n",
    "        validation_cols\n",
    "    )\n",
    "    \n",
    "    # Only include columns that actually exist\n",
    "    final_cols = [col for col in ordered_cols if col in formatted_bs.columns]\n",
    "    \n",
    "    return formatted_bs[final_cols]\n",
    "\n",
    "# Main execution\n",
    "# Load S&P 500 companies data\n",
    "# Load S&P 500 companies data\n",
    "sp500 = pd.read_csv('./data/sp500.csv')\n",
    "\n",
    "# Process all companies with specified directories\n",
    "process_sp500_companies(\n",
    "    sp500_df=sp500,\n",
    "    data_dir='/Users/maseehfaizan/Desktop/Maseeh/Projects/Hybrid_Pricer/data/clean', \n",
    "    mappings_dir='./data/XBRL_dic',  \n",
    "    output_dir='/Users/maseehfaizan/Desktop/Maseeh/Projects/Hybrid_Pricer/data/financial_statement'     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78549ca0",
   "metadata": {},
   "source": [
    "### Aggregatting all the accounts into a master account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064584ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to group files by ticker\n",
    "ticker_files = defaultdict(list)\n",
    "folder_path = \"./data/financial_statement\"\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Group files by ticker (part before first underscore), exclude master files\n",
    "for file in csv_files:\n",
    "    if '_' in file and not file.endswith('_master.csv'):\n",
    "        ticker = file.split('_')[0]\n",
    "        ticker_files[ticker].append(file)\n",
    "\n",
    "# Process each ticker group\n",
    "for ticker, files in ticker_files.items():\n",
    "    print(f\"Processing ticker: {ticker}\")\n",
    "    print(f\"Files: {files}\")\n",
    "    \n",
    "    # Read and merge all files for this ticker\n",
    "    merged_df = None\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if merged_df is None:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            # Merge on 'frame' column\n",
    "            merged_df = pd.merge(merged_df, df, on='frame', how='outer')\n",
    "    \n",
    "    # Remove metadata columns\n",
    "    metadata_cols = [col for col in merged_df.columns if any(col.startswith(meta) for meta in ['company_name', 'end', 'unit', 'form', 'cik'])]\n",
    "    merged_df = merged_df.drop(columns=metadata_cols)\n",
    "    \n",
    "    # Save merged dataframe\n",
    "    output_file = os.path.join(folder_path, f\"{ticker}_master.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved: {ticker}_master.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09141370",
   "metadata": {},
   "source": [
    "## Financial text (10-K) Parsing and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46e541",
   "metadata": {},
   "source": [
    "### Since I don't have the Capacity to process all the dataframe through an LLM I will only analyse 5 Sectors totalling 220 companies\n",
    "\n",
    "- it : Information Technology\n",
    "- cs : Communication Services\n",
    "- cd : Consumer Discretionary\n",
    "- consum : Sonsumer Staples\n",
    "- hc : Health Care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = sp[sp['GICS Sector']=='Information Technology']\n",
    "cs = sp[sp['GICS Sector']=='Communication Services']\n",
    "cd = sp[sp['GICS Sector']=='Consumer Discretionary']\n",
    "consum = sp[sp['GICS Sector']=='Consumer Staples']\n",
    "hc = sp[sp['GICS Sector']=='Health Care']\n",
    "df = pd.concat([it, cs, cd,consum, hc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1ef61",
   "metadata": {},
   "source": [
    "### SEC 10-K Parser\n",
    "\n",
    "This script scans a directory for local 10-K HTML files based on a list of company tickers. For each file, it:\n",
    "1.  Parses the HTML using `BeautifulSoup` to extract clean text.\n",
    "2.  Uses regular expressions (`re`) to locate and extract the content of specific sections (e.g., `Item 1. Business`, `Item 1A. Risk Factors`).\n",
    "3.  Appends the extracted `ticker`, `filing_date`, `section`, and `content` into a list.\n",
    "4.  Converts the aggregated data into a final pandas DataFrame named `long_df`.\n",
    "\n",
    "I have tried multiple ways of parsing this text data but creating the mapping like I did bellow works the best\n",
    "The code looks for Starting and ending patterns and grabs the text between (like a sandwich). The text with the most content wins (i.e gets captured and the rest is discarted) This way I don't capture text in the table of content and I don't grab any (reference text). I have noticed that for some companies they reference a section or an Item some place else so this is the most robust way to capture the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184abe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "# Configuration\n",
    "companies = list(df['Symbol'].values) # List of company tickers to process\n",
    "base_dir = './data/sec_filings/'\n",
    "\n",
    "# List to store data for all companies and all sections\n",
    "all_data = []\n",
    "\n",
    "# Section patterns and display names\n",
    "section_patterns = {\n",
    "    \"Business\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+1\\.\\s*', r'Item\\s+1\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+1A\\.\\s*', r'Item\\s+1A\\.\\s*', r'ITEM\\s+1\\.A\\.\\s*', r'Item\\s+1\\.A\\.\\s*', r'ITEM\\s+2\\.\\s*', r'Item\\s+2\\.\\s*'],\n",
    "        \"display_name\": \"Item 1. Business\"\n",
    "    },\n",
    "    \"Risk Factors\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+1A\\.\\s*', r'Item\\s+1A\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+1B\\.\\s*', r'Item\\s+1B\\.\\s*', r'ITEM\\s+1C\\.\\s*', r'Item\\s+1C\\.\\s*', r'ITEM\\s+2\\.\\s*', r'Item\\s+2\\.\\s*'],\n",
    "        \"display_name\": \"Item 1A. Risk Factors\"\n",
    "    },\n",
    "    \"Cybersecurity\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+1C\\.\\s*', r'Item\\s+1C\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+2\\.\\s*', r'Item\\s+2\\.\\s*'],\n",
    "        \"display_name\": \"Item 1C. Cybersecurity\"\n",
    "    },\n",
    "    \"Properties\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+2\\.\\s*', r'Item\\s+2\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+3\\.\\s*', r'Item\\s+3\\.\\s*'],\n",
    "        \"display_name\": \"Item 2. Properties\"\n",
    "    },\n",
    "    \"Legal Proceedings\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+3\\.\\s*', r'Item\\s+3\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+4\\.\\s*', r'Item\\s+4\\.\\s*'],\n",
    "        \"display_name\": \"Item 3. Legal Proceedings\"\n",
    "    },\n",
    "    \"Management Discussion and Analysis\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+7\\.\\s*', r'Item\\s+7\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+7A\\.\\s*', r'Item\\s+7A\\.\\s*', r'ITEM\\s+7\\.A\\.\\s*', r'Item\\s+7\\.A\\.\\s*'],\n",
    "        \"display_name\": \"Item 7. Management Discussion and Analysis\"\n",
    "    },\n",
    "    \"Quantitative and Qualitative Disclosures\": {\n",
    "        \"start_patterns\": [r'ITEM\\s+7A\\.\\s*', r'Item\\s+7A\\.\\s*'],\n",
    "        \"end_patterns\": [r'ITEM\\s+8\\.\\s*', r'Item\\s+8\\.\\s*'],\n",
    "        \"display_name\": \"Item 7A. Quantitative and Qualitative Disclosures about Market Risk\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Find all 10-K files for specified companies\n",
    "file_paths = []\n",
    "for ticker in companies:\n",
    "    pattern = f\"{base_dir}{ticker}_10-K_*.html\"\n",
    "    ticker_files = glob.glob(pattern)\n",
    "    file_paths.extend(ticker_files)\n",
    "\n",
    "if not file_paths:\n",
    "    print(\"No matching files found. Please check the directory and file naming pattern.\")\n",
    "else:\n",
    "    print(f\"Found {len(file_paths)} files to process.\")\n",
    "\n",
    "    # Process each file\n",
    "    for html_file_path in file_paths:\n",
    "        if not os.path.exists(html_file_path):\n",
    "            print(f\"Error: File not found at '{html_file_path}'\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Extract ticker from filename\n",
    "            base_filename = os.path.basename(html_file_path)\n",
    "            ticker = base_filename.split('_')[0]\n",
    "\n",
    "            # Extract date from filename\n",
    "            parts = base_filename.split('_')\n",
    "            filing_date = parts[2].split('.')[0] if len(parts) >= 3 else None\n",
    "\n",
    "            # Read the HTML file\n",
    "            with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "\n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "            # Extract Text\n",
    "            text_content = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Clean the text\n",
    "            text_lines = text_content.splitlines()\n",
    "            cleaned_lines = []\n",
    "            for line in text_lines:\n",
    "                processed_line = re.sub(r'[ \\t]+', ' ', line).strip()\n",
    "                if processed_line:\n",
    "                    cleaned_lines.append(processed_line)\n",
    "            final_text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "            # Sections to extract\n",
    "            section_names_to_extract = [\n",
    "                \"Business\",\n",
    "                \"Risk Factors\",\n",
    "                \"Cybersecurity\",\n",
    "                \"Properties\",\n",
    "                \"Legal Proceedings\",\n",
    "                \"Management Discussion and Analysis\",\n",
    "                \"Quantitative and Qualitative Disclosures\"\n",
    "            ]\n",
    "\n",
    "            # Extract all sections and store results\n",
    "            for section_name_key in section_names_to_extract:\n",
    "                section_config = section_patterns[section_name_key]\n",
    "                start_patterns = section_config[\"start_patterns\"]\n",
    "                end_patterns = section_config[\"end_patterns\"]\n",
    "                display_name = section_config[\"display_name\"]\n",
    "\n",
    "                valid_sections = []\n",
    "                for start_pattern in start_patterns:\n",
    "                    for start_match in re.finditer(start_pattern, final_text, re.IGNORECASE):\n",
    "                        start_pos = start_match.start()\n",
    "                        search_start = start_pos + len(start_match.group())\n",
    "\n",
    "                        for end_pattern in end_patterns:\n",
    "                            end_match = re.search(end_pattern, final_text[search_start:], re.IGNORECASE)\n",
    "                            if end_match:\n",
    "                                end_pos = search_start + end_match.start()\n",
    "                                section_content = final_text[start_pos:end_pos].strip()\n",
    "                                section_content = re.sub(start_pattern, '', section_content, flags=re.IGNORECASE).strip()\n",
    "\n",
    "                                min_content_length = 200\n",
    "                                if len(section_content) > min_content_length:\n",
    "                                    valid_sections.append({\n",
    "                                        'content': section_content,\n",
    "                                        'length': len(section_content),\n",
    "                                        'display_name': display_name\n",
    "                                    })\n",
    "                                break # Found an end pattern for this start, move to next start_match if any\n",
    "\n",
    "                if valid_sections:\n",
    "                    main_section = max(valid_sections, key=lambda x: x['length'])\n",
    "                    section_content_extracted = main_section['content']\n",
    "                    section_display_name = main_section['display_name']\n",
    "                    print(f\"Successfully extracted {section_display_name} section for {ticker} (filing date: {filing_date})\")\n",
    "                else:\n",
    "                    section_content_extracted = \"\"\n",
    "                    section_display_name = display_name\n",
    "                    print(f\"No {section_name_key} section found for {ticker}. Check the patterns or document structure.\")\n",
    "\n",
    "                all_data.append({\n",
    "                    'ticker': ticker,\n",
    "                    'filing_date': filing_date,\n",
    "                    'section': section_display_name,\n",
    "                    'content': section_content_extracted\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {html_file_path}: {e}\")\n",
    "\n",
    "# Create DataFrame with all sections\n",
    "if all_data:\n",
    "    long_df = pd.DataFrame(all_data)\n",
    "    print(\"Processing complete!\")\n",
    "else:\n",
    "    print(\"No data was extracted. Please check the file paths and contents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bedd20f",
   "metadata": {},
   "source": [
    "### The text data is now merged with the S&P 500 DataFrame. This makes sure that I have a well structured dataframe that is well aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with your long_df, get latest filing dates\n",
    "long_df['filing_date'] = pd.to_datetime(long_df['filing_date'])\n",
    "latest_dates = long_df.groupby('ticker')['filing_date'].max().reset_index()\n",
    "latest_filings = long_df.merge(latest_dates, on=['ticker', 'filing_date'])\n",
    "\n",
    "# Merge with sp dataframe on ticker/Symbol\n",
    "merged_df = latest_filings.merge(sp500, left_on='ticker', right_on='Symbol', how='left')\n",
    "\n",
    "# Drop rows where content is NaN\n",
    "merged_df = merged_df.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdcdfea",
   "metadata": {},
   "source": [
    "# GEMINI API  FOR Q&A, PORTER 5 FORCES AND 7 POWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb371e49",
   "metadata": {},
   "source": [
    "### I this part of the code the main goal is to parse text data and make sure they are analysable\n",
    "\n",
    "- First install and initialize GEMINI API \n",
    "- I am using the 2.5 Flash model with 0.01 temprature giving reproducible answers \n",
    "- I am making sure the answer is in JSON file Forcing the answer to be in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "# Configure the API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize the model with low temperature\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.5-flash-preview-05-20',\n",
    "    generation_config={\n",
    "        'temperature': 0.01, # Low temperature for more deterministic responses for reproducibility\n",
    "        'top_p': 0.95, # Top-p sampling for diversity\n",
    "        'response_mime_type': 'application/json'  # Force JSON output\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684add9",
   "metadata": {},
   "source": [
    "## Q&A given Text data\n",
    "\n",
    "I had prepared around 10 questions for each Item of the 10-K. I tried to make sure the questions are well designed for most companies and also make sure that they are relavant for each sections.\n",
    "\n",
    "This section was not used for the Thesis since I couldn't turn them into analysable data in the given timeframe\n",
    "\n",
    "This is computationally intensive and might not be the most efficient way of doing things!!!\n",
    "\n",
    "The code will send over the whole section with company name and the section title along side one question and will do it for the 10 Questions for that section per company. This is very Time and energy consuming\n",
    "\n",
    "The question mapping is a python file called `qustions.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca97b1",
   "metadata": {},
   "source": [
    "### In the following we are defining a function that will process dataframe with gemini. I will design the dataframe where one of the column will be the prompt for each company. I will itterate through that columns and save the results in a new column called gimini_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e544094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_with_gemini(df, file_name, prompt_column='question_prompt', max_retries=3):\n",
    "    responses = []\n",
    "    gemini_responses = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"Processing row {index + 1}/{len(df)}: {row['section']}\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                prompt = row[prompt_column]\n",
    "                \n",
    "                if row['content'] == '':\n",
    "                    response_text = \"\"\n",
    "                else:\n",
    "                    response = model.generate_content(prompt)\n",
    "                    response_text = response.text\n",
    "                \n",
    "                responses.append(response_text)\n",
    "                gemini_responses.append(response_text)\n",
    "                success = True\n",
    "                time.sleep(0.01)\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Error on attempt {retry_count}: {str(e)}\")\n",
    "                \n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = 65 if \"rate limit\" in str(e).lower() else 5\n",
    "                    print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    error_text = f\"Error after {retry_count} attempts: {str(e)}\"\n",
    "                    responses.append(error_text)\n",
    "                    gemini_responses.append(error_text)\n",
    "    \n",
    "    df['gemini_response'] = responses\n",
    "    \n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(gemini_responses, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from questions import company_10k_questions\n",
    "all_rows_with_questions = []\n",
    "\n",
    "# Iterate over each row in the merged_df\n",
    "# Using itertuples(index=False) and zipping with columns for robust column name handling\n",
    "for row_tuple in merged_df.itertuples(index=False):\n",
    "    # Create a dictionary from the row tuple using original column names\n",
    "    original_row_dict = dict(zip(merged_df.columns, row_tuple))\n",
    "\n",
    "    company_name = original_row_dict.get('Security', 'Unknown Company') # Default if 'Security' is missing\n",
    "    section_from_df = original_row_dict.get('section')\n",
    "\n",
    "    if section_from_df is None:\n",
    "        # Skip row if 'section' column is missing or None\n",
    "        continue\n",
    "\n",
    "    # Normalize the section name from DataFrame to match dictionary keys\n",
    "    # (e.g., \"Item 1. Business\" to \"Item 1: Business\")\n",
    "    section_from_df = section_from_df.strip()\n",
    "    dict_key = section_from_df.replace(\". \", \": \", 1)\n",
    "\n",
    "    questions_list = company_10k_questions.get(dict_key)\n",
    "\n",
    "    if questions_list:\n",
    "        for question_template in questions_list:\n",
    "            # Format the question with the company name\n",
    "            formatted_question = question_template.format(company_name=company_name)\n",
    "            \n",
    "            # Create a new dictionary for the new row, copying original data\n",
    "            new_row_entry = original_row_dict.copy()\n",
    "            new_row_entry['Formatted_Question'] = formatted_question\n",
    "            \n",
    "            all_rows_with_questions.append(new_row_entry)\n",
    "    else:\n",
    "        # Optionally handle cases where a section in df doesn't match any key in questions dict\n",
    "        print(f\"No questions found for section: {section_from_df} (mapped to {dict_key})\")\n",
    "\n",
    "\n",
    "# Create the new DataFrame with each question as a row\n",
    "expanded_df = pd.DataFrame(all_rows_with_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adbc6f",
   "metadata": {},
   "source": [
    "### Question & Answer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50219218",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df['question_prompt'] = (\n",
    " \"As financial analysts, we are extracting financial data from the 10-K, more specifically the \" +\n",
    "expanded_df['section'] + \" section of the 10-K for the company \" + expanded_df['Security'] +\n",
    "\", which is generally operating in the \" + expanded_df['GICS Sector'] +\n",
    "\" GICS Sector, specifically the \" + expanded_df['GICS Sub-Industry'] + \" GICS Sub-Industry.\\n\" +\n",
    "\"You are an information extraction bot. **Strictly adhere to the text in the \\\"\" + expanded_df['section'] +\n",
    "\"\\\" section to answer the questions below.**\\n\" +\n",
    "\"**IMPORTANT: Your response must be ONLY a valid JSON object in this exact format:**\\n\" +\n",
    "\"{\\n\" +\n",
    "\" \\\"question\\\": \\\"\" + expanded_df['Formatted_Question'] + \"\\\",\\n\" +\n",
    "\" \\\"answer\\\": \\\"[Your detailed paragraph answer here]\\\",\\n\" +\n",
    "\" \\\"supporting_quote\\\": \\\"[Exact quote from the text that supports your answer]\\\",\\n\" +\n",
    "\" \\\"swot_category\\\": \\\"[One of: Strength, Weakness, Opportunity, Threat]\\\",\\n\" +\n",
    "\" \\\"confidence\\\": [Your confidence score from 0.0 to 1.0]\\n\" +\n",
    "\"}\\n\" +\n",
    "\"**Guidelines:**\\n\" +\n",
    "\"- Answer must be a complete paragraph, no bullet points or internal lists\\n\" +\n",
    "\"- Supporting quote must be an exact excerpt from the provided text\\n\" +\n",
    "\"- SWOT category determination:\\n\" +\n",
    "\"  - **Strength**: Positive internal factors that give the company advantages (e.g., strong market position, unique capabilities, competitive advantages)\\n\" +\n",
    "\"  - **Weakness**: Negative internal factors that put the company at a disadvantage (e.g., operational inefficiencies, resource limitations, competitive disadvantages)\\n\" +\n",
    "\"  - **Opportunity**: Positive external factors the company could exploit (e.g., market growth, emerging trends, regulatory changes that benefit)\\n\" +\n",
    "\"  - **Threat**: Negative external factors that could harm the company (e.g., competition, regulatory risks, market decline, economic headwinds)\\n\" +\n",
    "\"- Confidence should reflect how directly the information answers the question (1.0 = perfect match, 0.0 = no relevant information)\\n\" +\n",
    "\"- If information is not explicitly present, set answer to \\\"Information not available in this section.\\\", swot_category to \\\"Not Applicable\\\", and confidence to 0.0\\n\" +\n",
    "\"- Do not include any text outside the JSON object\\n\" +\n",
    "\"GICS Sector: \" + expanded_df['GICS Sector'] + \"\\n\" +\n",
    "\"GICS Sub-Industry: \" + expanded_df['GICS Sub-Industry'] + \"\\n\" +\n",
    "\"**\" + expanded_df['section'] + \" Text:**\\n\" +\n",
    "expanded_df['content'] + \"\\n\" +\n",
    "\"---\\n\" +\n",
    "\"**Question:** \" + expanded_df['Formatted_Question']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797a57c",
   "metadata": {},
   "source": [
    "### Use the code for the Q&A and Keep the answers there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Q&A SWOT analysis\n",
    "file_name = 'gemini_responses.json'\n",
    "df_QA_processed = process_dataframe_with_gemini(expanded_df, file_name, 'question_prompt')\n",
    "df_QA_processed.to_csv('./data/cs_questions_answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61b85e",
   "metadata": {},
   "source": [
    "## Same Analysis for Porter 5 Forces\n",
    "\n",
    "### I have prepared a JSON file with Porter 5 Forces after analyzing the book so the LLM knows exactly what to chose from when analyzing the companies and their reports section \n",
    "\n",
    "- Here I will only pass through section by section which is a more efficient way but MIGHT result into loss\n",
    "\n",
    "- I have also explicitly asked for a JSON file and made sure the Model doesn't halucinate and stays grounder\n",
    "- Finally I have asked for Quotes and Justifications where I can verify for a Sub-Sample that the analysis was well performed (Of course I couldn't check for all the answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67aeddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert the Porter's framework JSON to a string if it's not already\n",
    "with open('Porter.json', 'r') as f:\n",
    "    porters_framework_json_str = f.read() # Assuming you have the JSON object\n",
    "\n",
    "merged_df['porter_prompt'] = (\n",
    "    \"You are a financial analyst engine. Your task is to analyze a section of text extracted from \" +\n",
    "    merged_df['Security'] + \"'s 10-K filing and evaluate how it relates to ALL of Porter's Five Forces, \" +\n",
    "    \"determining the relevance and threat/power level for each force.\\n\\n\" +\n",
    "    \"**Context:**\\n\" +\n",
    "    \"- Company: \" + merged_df['Security'] + \"\\n\" +\n",
    "    \"- GICS Sector: \" + merged_df['GICS Sector'] + \"\\n\" +\n",
    "    \"- GICS Sub-Industry: \" + merged_df['GICS Sub-Industry'] + \"\\n\" +\n",
    "    \"- Section: \" + merged_df['section'] + \"\\n\\n\" +\n",
    "    \"**Your Goal:**\\n\" +\n",
    "    \"Analyze the provided section and assess how it relates to each of Porter's Five Forces, providing confidence scores and threat levels for all five forces.\\n\\n\" +\n",
    "    \"**IMPORTANT: Your response MUST be ONLY a valid JSON object in this exact format:**\\n\" +\n",
    "    \"{\\n\" +\n",
    "    \"  \\\"analysis\\\": {\\n\" +\n",
    "    \"    \\\"threat_of_new_entrants\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0 indicating how well the text relates to this force]\\\",\\n\" +\n",
    "    \"      \\\"threat_level\\\": \\\"[Either 'High', 'Low', or 'Not Applicable' based on the conditions described]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text that support this analysis]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation of the fit score and threat level, referencing specific conditions from the framework]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"bargaining_power_of_buyers\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"threat_level\\\": \\\"[Either 'High', 'Low', or 'Not Applicable']\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"bargaining_power_of_suppliers\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"threat_level\\\": \\\"[Either 'High', 'Low', or 'Not Applicable']\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"threat_of_substitute_products\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"threat_level\\\": \\\"[Either 'High', 'Low', or 'Not Applicable']\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"intensity_of_rivalry\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"threat_level\\\": \\\"[Either 'High', 'Low', or 'Not Applicable']\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    }\\n\" +\n",
    "    \"  },\\n\" +\n",
    "    \"  \\\"primary_force\\\": \\\"[The force with the highest confidence_of_fit score]\\\",\\n\" +\n",
    "    \"  \\\"overall_summary\\\": \\\"[Brief summary of how this section relates to the company's competitive position]\\\"\\n\" +\n",
    "    \"}\\n\\n\" +\n",
    "    \"**Instructions:**\\n\" +\n",
    "    \"1. Read the entire section carefully and identify all competitive dynamics mentioned.\\n\" +\n",
    "    \"2. For EACH of the five forces:\\n\" +\n",
    "    \"   - Assign a confidence_of_fit score (0.0 = no relevance, 1.0 = perfect fit)\\n\" +\n",
    "    \"   - Determine the threat_level based on conditions described in the text\\n\" +\n",
    "    \"   - Extract exact quotes that support your analysis\\n\" +\n",
    "    \"   - Provide detailed justification referencing specific conditions from the framework\\n\" +\n",
    "    \"3. Use \\\"Not Applicable\\\" for threat_level when confidence_of_fit is below 0.2\\n\" +\n",
    "    \"4. Quote EXACT words and phrases from the text - do not paraphrase\\n\" +\n",
    "    \"5. Reference specific conditions from the PORTERS_FRAMEWORK_JSON in your justifications\\n\\n\" +\n",
    "    \"**Scoring Guidelines:**\\n\" +\n",
    "    \"- 0.0-0.2: No clear relevance to this force\\n\" +\n",
    "    \"- 0.3-0.5: Some indirect relevance or implications\\n\" +\n",
    "    \"- 0.6-0.8: Clear relevance with specific examples\\n\" +\n",
    "    \"- 0.9-1.0: Direct discussion of this force with multiple specific conditions\\n\\n\" +\n",
    "    \"**Important Analysis Guidelines:**\\n\" +\n",
    "    \"- Analyze ALL five forces, even if some have low relevance\\n\" +\n",
    "    \"- Use exact quotes to support your analysis\\n\" +\n",
    "    \"- Consider both explicit mentions and implicit implications\\n\" +\n",
    "    \"- Be specific about which conditions from the framework apply\\n\" +\n",
    "    \"- Assess threat levels based on whether described conditions increase or decrease competitive pressures\\n\\n\" +\n",
    "    \"---\\n\" +\n",
    "    \"**PORTERS_FRAMEWORK_JSON:**\\n\" +\n",
    "    porters_framework_json_str + \"\\n\" +\n",
    "    \"---\\n\" +\n",
    "    \"**\" + merged_df['section'] + \" Text:**\\n\" +\n",
    "    merged_df['content'] + \"\\n\" +\n",
    "    \"---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aaa4ca",
   "metadata": {},
   "source": [
    "### Using the same function as before for porter's 5 Forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Porter's analysis\n",
    "file_name_porter = 'gemini_porter_responses.json'\n",
    "df_processed_porter = process_dataframe_with_gemini(merged_df, file_name_porter, 'porter_prompt')\n",
    "df_processed_porter.to_csv('./data/df_porter_analysis_results_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88f5ca",
   "metadata": {},
   "source": [
    "## Same analysis for Hamilton's 7 Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 7 Powers framework from the JSON file\n",
    "with open('SevenPower.json', 'r') as f:\n",
    "    seven_powers_json_str = f.read()\n",
    "\n",
    "merged_df['seven_powers_prompt'] = (\n",
    "    \"You are a strategic business analyst specializing in Hamilton Helmer's 7 Powers framework. \" +\n",
    "    \"Your task is to analyze a section of text extracted from \" + merged_df['Security'] + \n",
    "    \"'s 10-K filing and evaluate how it relates to ALL of the 7 Powers.\\n\\n\" +\n",
    "    \"**Context:**\\n\" +\n",
    "    \"- Company: \" + merged_df['Security'] + \"\\n\" +\n",
    "    \"- GICS Sector: \" + merged_df['GICS Sector'] + \"\\n\" +\n",
    "    \"- GICS Sub-Industry: \" + merged_df['GICS Sub-Industry'] + \"\\n\" +\n",
    "    \"- Section: \" + merged_df['section'] + \"\\n\\n\" +\n",
    "    \"**Your Goal:**\\n\" +\n",
    "    \"Analyze the provided section and assess how it relates to each of Hamilton's 7 Powers, providing confidence scores and strength assessments for all seven powers.\\n\\n\" +\n",
    "    \"**IMPORTANT: Your response MUST be ONLY a valid JSON object in this exact format:**\\n\" +\n",
    "    \"{\\n\" +\n",
    "    \"  \\\"analysis\\\": {\\n\" +\n",
    "    \"    \\\"scale_economies\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0 indicating how well the text relates to this power]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"['Strength' if power is present, 'Weakness' if absent but needed, 'Opportunity' if emerging, 'Threat' if competitors have it]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text that support this analysis]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence from the text]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation of the fit score and strength assessment]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"network_economies\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"[Strength/Weakness/Opportunity/Threat]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"counter_positioning\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"[Strength/Weakness/Opportunity/Threat]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"switching_costs\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"[Strength/Weakness/Opportunity/Threat]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"branding\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"[Strength/Weakness/Opportunity/Threat]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"cornered_resource\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"[Strength/Weakness/Opportunity/Threat]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    },\\n\" +\n",
    "    \"    \\\"process_power\\\": {\\n\" +\n",
    "    \"      \\\"confidence_of_fit\\\": \\\"[Score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"power_strength\\\": \\\"[Numerical score from 0.0 to 1.0]\\\",\\n\" +\n",
    "    \"      \\\"swot_classification\\\": \\\"[Strength/Weakness/Opportunity/Threat]\\\",\\n\" +\n",
    "    \"      \\\"relevant_quotes\\\": [\\\"[Exact quotes from the text]\\\"],\\n\" +\n",
    "    \"      \\\"key_indicators\\\": [\\\"[Specific evidence]\\\"],\\n\" +\n",
    "    \"      \\\"justification\\\": \\\"[Detailed explanation]\\\"\\n\" +\n",
    "    \"    }\\n\" +\n",
    "    \"  },\\n\" +\n",
    "    \"  \\\"primary_power\\\": \\\"[The power with the highest confidence_of_fit score]\\\",\\n\" +\n",
    "    \"  \\\"power_timing\\\": \\\"[Based on primary power: 'Origination', 'Takeoff', 'Stability', or 'N/A']\\\",\\n\" +\n",
    "    \"  \\\"overall_summary\\\": \\\"[Brief summary of the company's competitive position based on the 7 Powers analysis]\\\"\\n\" +\n",
    "    \"}\\n\\n\" +\n",
    "    \"**Instructions:**\\n\" +\n",
    "    \"1. Read the entire section carefully and identify all strategic elements mentioned.\\n\" +\n",
    "    \"2. For EACH of the seven powers:\\n\" +\n",
    "    \"   - Assign a confidence_of_fit score (0.0 = no relevance, 1.0 = perfect fit)\\n\" +\n",
    "    \"   - Assign a power_strength score as a number from 0.0 to 1.0\\n\" +\n",
    "    \"   - Classify as Strength/Weakness/Opportunity/Threat for SWOT analysis\\n\" +\n",
    "    \"   - Extract exact quotes that support your analysis\\n\" +\n",
    "    \"   - List key indicators that demonstrate this power\\n\" +\n",
    "    \"   - Provide detailed justification referencing the framework\\n\" +\n",
    "    \"3. Quote EXACT words and phrases from the text - do not paraphrase\\n\" +\n",
    "    \"4. Reference specific definitions, benefits, and barriers from the SEVEN_POWERS_JSON\\n\\n\" +\n",
    "    \"**Scoring Guidelines:**\\n\" +\n",
    "    \"- confidence_of_fit:\\n\" +\n",
    "    \"  - 0.0-0.2: No clear relevance to this power\\n\" +\n",
    "    \"  - 0.3-0.5: Some indirect relevance or weak indicators\\n\" +\n",
    "    \"  - 0.6-0.8: Clear relevance with specific examples\\n\" +\n",
    "    \"  - 0.9-1.0: Direct evidence of this power with multiple indicators\\n\\n\" +\n",
    "    \"- power_strength (numerical):\\n\" +\n",
    "    \"  - 0.0-0.2: Not Present - No meaningful evidence of this power\\n\" +\n",
    "    \"  - 0.2-0.4: Weak - Some indicators present but limited evidence of real competitive advantage\\n\" +\n",
    "    \"  - 0.4-0.6: Emerging - Early signs that this power is developing\\n\" +\n",
    "    \"  - 0.6-0.8: Moderate - Good evidence but perhaps not fully developed or facing some challenges\\n\" +\n",
    "    \"  - 0.8-1.0: Strong - Clear evidence of an established power with significant competitive advantage\\n\\n\" +\n",
    "    \"**SWOT Classification:**\\n\" +\n",
    "    \"- \\\"Strength\\\": Company clearly possesses this power\\n\" +\n",
    "    \"- \\\"Weakness\\\": Company lacks this power but could benefit from it\\n\" +\n",
    "    \"- \\\"Opportunity\\\": Early indicators suggest this power could be developed\\n\" +\n",
    "    \"- \\\"Threat\\\": Competitors appear to have this power advantage\\n\\n\" +\n",
    "    \"**Important Analysis Guidelines:**\\n\" +\n",
    "    \"- Analyze ALL seven powers, even if some have low relevance\\n\" +\n",
    "    \"- Look for evidence of sustainable competitive advantages\\n\" +\n",
    "    \"- Consider the specific benefit and barrier for each power\\n\" +\n",
    "    \"- Identify specific metrics, market positions, or strategic elements\\n\" +\n",
    "    \"- Be precise about which aspects of each power are demonstrated\\n\\n\" +\n",
    "    \"---\\n\" +\n",
    "    \"**SEVEN_POWERS_JSON:**\\n\" +\n",
    "    seven_powers_json_str + \"\\n\" +\n",
    "    \"---\\n\" +\n",
    "    \"**\" + merged_df['section'] + \" Text:**\\n\" +\n",
    "    merged_df['content'] + \"\\n\" +\n",
    "    \"---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hamilton's Seven Power analysis\n",
    "file_name_power = 'gemini_power_responses_1.json'\n",
    "df_processed_power = process_dataframe_with_gemini(merged_df, file_name_power, 'seven_powers_prompt')\n",
    "df_processed_power.to_csv('./data/df_power_analysis_results_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03193222",
   "metadata": {},
   "source": [
    "## Reddit Text data\n",
    "\n",
    "Finally performed the same analysis for Reddit data but first we need to load cleaned Reddit data clean it a bit further before analysing it with GEMINI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a6c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv('./data/reddit/cleaned_stock.csv')\n",
    "reddit = reddit.dropna(subset=['post', 'comments'])\n",
    "\n",
    "## Convert created_utc to datetime \n",
    "reddit['created_utc'] = pd.to_datetime(reddit['created_utc'])\n",
    "\n",
    "# Extract year from created_utc\n",
    "reddit['year'] = reddit['created_utc'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baabf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with only 2018+ data\n",
    "reddit = reddit[reddit['year'] >= 2018].copy()\n",
    "# Now create the plot with cleaned data\n",
    "yearly_dist = reddit.groupby('year').size().sort_index()\n",
    "yearly_dist.plot(kind='bar')\n",
    "plt.title('Number of Posts by Year (2020 onwards)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d168e9",
   "metadata": {},
   "source": [
    "### Merging the Reddit data with the main `df` dataframe with all the companies This will remove all the companies we are not analyzing and focus on the companies that we are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35124ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df and reddit dataframes on Symbol and search_term columns\n",
    "reddit_merge = pd.merge(df, reddit, \n",
    "                    left_on='Symbol',\n",
    "                    right_on='search_term',\n",
    "                    how='inner')\n",
    "#  Drop unnecessary columns\n",
    "reddit_merge = reddit_merge.drop(columns=['search_term','Unnamed: 0.1','Unnamed: 0_x','Unnamed: 0_y','permalink'])\n",
    "# Drop rows where 'post' or 'comments' are NaN\n",
    "reddit_merge = reddit_merge.dropna(subset = ['post','comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ed35e",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18149832",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_merge['reddit_sentiment_prompt'] = (\n",
    "    \"**Role:** You are an expert financial analyst specializing in social media sentiment and alternative data.\\n\\n\" +\n",
    "    \"**Objective:** The following data is for the stock ticker **\" + reddit_merge['Symbol'] + \"**. \" +\n",
    "    \"Analyze the Reddit post and its associated comments to provide a structured summary of the discussion for a machine learning model.\\n\\n\" +\n",
    "    \"**Input Data:**\\n\" +\n",
    "    \"**Ticker:** \" + reddit_merge['Symbol'] + \"\\n\" +\n",
    "    \"**Post Title:** \" + reddit_merge['title'] + \"\\n\" +\n",
    "    \"**Post Body:** \" + reddit_merge['post'] + \"\\n\" +\n",
    "    \"**Upvote Ratio:** \" + reddit_merge['upvote_ratio'].astype(str) + \"\\n\" +\n",
    "    \"**Number of Comments:** \" + reddit_merge['num_comments'].astype(str) + \"\\n\" +\n",
    "    \"**Comments:** \" + reddit_merge['comments'] + \"\\n\\n\" +  \n",
    "    \"**Task:** Based on the input data, generate a single JSON object that structures the discussion's content and sentiment. \" +\n",
    "    \"The analysis should clearly distinguish between the original post's claims and the community's response in the comments.\\n\\n\" +\n",
    "    \"**JSON Output Schema:**\\n\" +\n",
    "    \"{\\n\" +\n",
    "    \"  \\\"post_summary\\\": {\\n\" +\n",
    "    \"    \\\"thesis\\\": \\\"Summarize the main point, argument, or question of the original post. What is the author claiming or asking?\\\",\\n\" +\n",
    "    \"    \\\"evidence_type\\\": \\\"Classify the type of evidence used in the post (e.g., 'Official Financials', 'Technical Analysis', 'Speculation', 'News Article', 'Personal Opinion').\\\"\\n\" +\n",
    "    \"  },\\n\" +\n",
    "    \"  \\\"comments_summary\\\": {\\n\" +\n",
    "    \"    \\\"main_theme\\\": \\\"Summarize the general sentiment and key themes of the comments. Are they in agreement or disagreement with the post?\\\",\\n\" +\n",
    "    \"    \\\"counter_arguments\\\": \\\"List the most significant counter-arguments or bearish points raised in the comments.\\\",\\n\" +\n",
    "    \"    \\\"corroborating_points\\\": \\\"List the most significant points from the comments that support or agree with the original post.\\\"\\n\" +\n",
    "    \"  },\\n\" +\n",
    "    \"  \\\"quantitative_analysis\\\": {\\n\" +\n",
    "    \"    \\\"sentiment_score\\\": \\\"[A float score from -1.0 (very bearish) to 1.0 (very bullish) for the entire discussion]\\\",\\n\" +\n",
    "    \"    \\\"sentiment_reasoning\\\": \\\"A brief explanation for the sentiment score, citing the balance between the post and the comments.\\\",\\n\" +\n",
    "    \"    \\\"conviction_level\\\": \\\"[Rate as 'Low', 'Medium', or 'High' based on the certainty and language used across the entire discussion.]\\\",\\n\" +\n",
    "    \"    \\\"predominant_emotion\\\": \\\"[e.g., 'Analytical', 'Fear', 'Greed', 'Uncertainty', 'Hope']\\\"\\n\" +\n",
    "    \"  },\\n\" +\n",
    "    \"  \\\"market_outlook\\\": \\\"[Classify the overall conclusive outlook for the ticker from this discussion as 'Bullish', 'Bearish', 'Neutral', or 'Contentious']\\\"\\n\" +\n",
    "    \"}\\n\\n\" +\n",
    "    \"**Instructions:**\\n\" +\n",
    "    \"1. The analysis must focus only on the provided ticker: **\" + reddit_merge['Symbol'] + \"**.\\n\" +\n",
    "    \"2. Clearly separate the analysis of the Post Body from the Comments.\\n\" +\n",
    "    \"3. market_outlook should be 'Contentious' if there is strong disagreement between the post and comments or within the comments themselves.\\n\" +\n",
    "    \"4. Consider the upvote ratio (\" + reddit_merge['upvote_ratio'].astype(str) + \") as an indicator of community agreement with the post.\\n\" +\n",
    "    \"5. Note that there are \" + reddit_merge['num_comments'].astype(str) + \" comments on this post.\\n\" +\n",
    "    \"6. Provide the final output in a single JSON block without any additional commentary.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb183677",
   "metadata": {},
   "source": [
    "### Designing a VERY similar function as before but for Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def process_reddit_with_gemini(df, file_name, prompt_column='reddit_sentiment_prompt', max_retries=3):\n",
    "    responses = []\n",
    "    gemini_responses = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Display progress with symbol and title (or just index if not available)\n",
    "        display_text = f\"{row['symbol']} - {row['title'][:50]}...\" if 'symbol' in row and 'title' in row else f\"Row {index}\"\n",
    "        print(f\"Processing {index + 1}/{len(df)}: {display_text}\")\n",
    "        \n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                prompt = row[prompt_column]\n",
    "                \n",
    "                response = model.generate_content(prompt)\n",
    "                response_text = response.text\n",
    "                \n",
    "                responses.append(response_text)\n",
    "                gemini_responses.append(response_text)\n",
    "                success = True\n",
    "                time.sleep(0.01)\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"Error on attempt {retry_count}: {str(e)}\")\n",
    "                \n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = 65 if \"rate limit\" in str(e).lower() else 5\n",
    "                    print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    error_text = f\"Error after {retry_count} attempts: {str(e)}\"\n",
    "                    responses.append(error_text)\n",
    "                    gemini_responses.append(error_text)\n",
    "    \n",
    "    df['gemini_response'] = responses\n",
    "    \n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        json.dump(gemini_responses, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# For Reddit analysis\n",
    "file_name_reddit = 'gemini_reddit_analysis.json'\n",
    "df_reddit_processed = process_reddit_with_gemini(reddit_merge, file_name_reddit, 'reddit_sentiment_prompt')\n",
    "df_reddit_processed.to_csv('./data/reddit_analysis_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bd0d3",
   "metadata": {},
   "source": [
    "## Using GEMINI API for Financial Statements and DCF Accounts preparations for different sub-industries in given sector\n",
    "\n",
    "Here I am categorizing different Industry and different analysis metrics. \n",
    "- In Information Technology the norme is to compute the **DCF** Model\n",
    "- In **Financials** We need to compute the *Dividend Discount Model* **DDM**\n",
    "- In **Real Estate** we need Net Asset Values **NAV**\n",
    "\n",
    "This is how we will categorize and price these different companies in different sector because this is how a fundamental analyst would have done so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dcf = ['Information Technology','Communication Services','Consumer Discretionary','Consumer Staples','Health Care','Industrials','Materials']\n",
    "ddm = ['Utilities','Financials']\n",
    "nav = ['Real Estate','Energy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54fd5b",
   "metadata": {},
   "source": [
    "### Making sure I have one company per Sub-Industry Because as I did before all the accounts are mapped at the sub-industry level coming from EDGAR API. This means we simply need to map DCF accounts at the Sub-industry level as well \n",
    "\n",
    "OUTPUT ACCOUNT `dcf.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = sp[sp['GICS Sector']=='Information Technology']\n",
    "cs = sp[sp['GICS Sector']=='Communication Services']\n",
    "cd = sp[sp['GICS Sector']=='Consumer Discretionary']\n",
    "css = sp[sp['GICS Sector']=='Consumer Staples']\n",
    "hc = sp[sp['GICS Sector']=='Health Care']\n",
    "ind = sp[sp['GICS Sector']=='Industrials']\n",
    "mat = sp[sp['GICS Sector']=='Materials']\n",
    "df = pd.concat([it, cs, cd,css,hc,ind,mat], ignore_index=True)\n",
    "# After your existing code\n",
    "df_one_per_subindustry = df.groupby('GICS Sub-Industry').first().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db904821",
   "metadata": {},
   "source": [
    "### Support code for DCF Account creation with prompt and a way to make sure the prompt is generated and is in proper columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sp_analysis = df_one_per_subindustry.copy()\n",
    "\n",
    "# Initialize counter and accounts storage\n",
    "ix = 0\n",
    "sp_analysis['accounts'] = None\n",
    "sp_analysis['question_prompt'] = None\n",
    "\n",
    "# Process each company\n",
    "for idx, row in sp_analysis.iterrows():\n",
    "    symbol = row['Symbol']\n",
    "    sub_industry = row['GICS Sub-Industry']\n",
    "    \n",
    "    # Retry configuration\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    \n",
    "    while retry_count < max_retries and not success:\n",
    "        try:\n",
    "            print(f'Processing {symbol} (Attempt {retry_count + 1}/{max_retries})')\n",
    "            \n",
    "            # Read the financial statement\n",
    "            df = pd.read_csv(f'./data/financial_statement/{symbol}_master.csv')\n",
    "            accounts = df.columns.to_list()\n",
    "            \n",
    "            # Store accounts in the dataframe\n",
    "            sp_analysis.at[idx, 'accounts'] = accounts\n",
    "            \n",
    "            # Create the prompt\n",
    "            accounts_str = ', '.join(accounts)\n",
    "            prompt = (\n",
    "                \"Hey Gemini I want a concise answer. \\n\" +\n",
    "                \" The Goal is to create a standard DCF model using the mappings you will provide. Take into account the account names for the specific industry and create a mapping code like shown in the example bellow \"+\n",
    "                \"Make sure the answer is given in JSON format \"+\n",
    "                \"\\n ------ \\n\" +\n",
    "                f\"Here are account names for companies that are in {sub_industry} sub-industry. \\n\" +\n",
    "                accounts_str + \"I want these accounts to be mapped like this: \\n\" +\n",
    "                \"\"\"I want you to use these accounts and create a mapping like this\n",
    "                Make sure to always write Sub-Industry name in Pascal format: .... for example {\n",
    "                \"**ApplicationSoftware**\": {\n",
    "                \"revenue\": [\n",
    "                \"IncomeStatement - Revenues - TotalRevenue\"\n",
    "                ],\n",
    "                \"cogs\": [\n",
    "                \"IncomeStatement - Expenses - CostOfRevenue\"\n",
    "                ],\n",
    "                \"operating_expenses\": [\n",
    "                \"IncomeStatement - Expenses - SalesAndMarketing\",\n",
    "                \"IncomeStatement - Expenses - GeneralAndAdministrative\"\n",
    "                ],\n",
    "                \"d_and_a\": [\n",
    "                \"CashFlow - OperatingActivities - DepreciationAndAmortization\"\n",
    "                ],\n",
    "                \"capex\": [\n",
    "                \"CashFlow - InvestingActivities - CapitalExpenditures\"\n",
    "                ],\n",
    "                \"stock_based_compensation\": [\n",
    "                \"CashFlow - OperatingActivities - ShareBasedCompensation\"\n",
    "                ],\n",
    "                \"nwc_operating_assets\": [\n",
    "                \"Assets - AccountsReceivable\",\n",
    "                \"Assets - ContractAssets\",\n",
    "                \"Assets - CapitalizedContractCosts\",\n",
    "                \"Assets - PrepaidExpenses\"\n",
    "                ],\n",
    "                \"nwc_operating_liabilities\": [\n",
    "                \"Liabilities - AccountsPayable\",\n",
    "                \"Liabilities - AccruedLiabilities\",\n",
    "                \"Liabilities - DeferredRevenue\"\n",
    "                ]\n",
    "                }\n",
    "                }\n",
    "                Make sure the answer is just the mapping no introduction sentence no ending sentence just the mapping code\"\"\"+ \"\"\"Don't add any other comments i.e ( // Note: DepreciationAndAmortization, ShareBasedCompensation, and RestructuringCharges are typically excluded\n",
    "                // from core operating expenses for normalized free cash flow calculation.)\"\"\"\n",
    "                            )\n",
    "            \n",
    "            sp_analysis.at[idx, 'question_prompt'] = str(prompt)\n",
    "            \n",
    "            # Mark as successful\n",
    "            success = True\n",
    "            print(f'Successfully processed {symbol}')\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found for {symbol}. Skipping...')\n",
    "            # Don't retry for file not found errors\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f'Error processing {symbol}: {e}')\n",
    "            \n",
    "            if retry_count < max_retries:\n",
    "                print(f'Waiting 60 seconds before retry...')\n",
    "                time.sleep(60)  # Wait 1 minute before retrying\n",
    "            else:\n",
    "                print(f'Max retries reached for {symbol}. Moving to next company.')\n",
    "                # Optionally store error information\n",
    "                sp_analysis.at[idx, 'error'] = str(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ebd4d",
   "metadata": {},
   "source": [
    "### MAIN DCF JSON CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0afb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extract JSON content from Gemini's response, handling markdown code blocks\n",
    "    \"\"\"\n",
    "    # Try to find JSON content within ```json ... ``` blocks\n",
    "    json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(json_pattern, response_text, re.DOTALL)\n",
    "    \n",
    "    if matches:\n",
    "        # Take the first match\n",
    "        json_str = matches[0].strip()\n",
    "    else:\n",
    "        # If no markdown blocks, assume the entire response is JSON\n",
    "        json_str = response_text.strip()\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON string\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"JSON string attempted: {json_str[:200]}...\")  # Show first 200 chars\n",
    "        return None\n",
    "\n",
    "def process_dataframe_with_gemini(df, output_file='gemini_responses.json'):\n",
    "    \"\"\"\n",
    "    Process each row's prompt and save Gemini's response as proper JSON\n",
    "    \"\"\"\n",
    "    # Dictionary to store all responses\n",
    "    all_responses = {}\n",
    "    response_texts = []  # For dataframe column\n",
    "    \n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            prompt = row['question_prompt']\n",
    "            sub_industry = row['GICS Sub-Industry']\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing {sub_industry}\")\n",
    "            \n",
    "            # Send prompt to Gemini\n",
    "            response = model.generate_content(prompt)\n",
    "            response_text = response.text\n",
    "            \n",
    "            print('Response received')\n",
    "            \n",
    "            # Parse JSON from response\n",
    "            parsed_json = parse_json_from_response(response_text)\n",
    "            \n",
    "            if parsed_json:\n",
    "                # If the parsed JSON has a single key (like the examples show),\n",
    "                # merge it into our main dictionary\n",
    "                if isinstance(parsed_json, dict) and len(parsed_json) == 1:\n",
    "                    all_responses.update(parsed_json)\n",
    "                else:\n",
    "                    # Otherwise, use the sub-industry as the key\n",
    "                    all_responses[sub_industry] = parsed_json\n",
    "                \n",
    "                print(f\"Successfully parsed JSON for {sub_industry}\")\n",
    "                response_texts.append(json.dumps(parsed_json))\n",
    "            else:\n",
    "                print(f\"Failed to parse JSON for {sub_industry}\")\n",
    "                all_responses[sub_industry] = {\"error\": \"Failed to parse JSON\", \"raw_response\": response_text[:500]}\n",
    "                response_texts.append(response_text)\n",
    "            \n",
    "            print(f\"{'='*50}\\n\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            print(f\"Error processing row {row['GICS Sub-Industry']}: {str(e)}\")\n",
    "            \n",
    "            # Store error in the responses\n",
    "            all_responses[row['GICS Sub-Industry']] = {\"error\": str(e)}\n",
    "            response_texts.append(error_msg)\n",
    "            \n",
    "            time.sleep(60)  # Longer wait on error\n",
    "    \n",
    "    # Write the combined JSON to file\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_responses, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n✓ Valid JSON file created: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error writing JSON file: {e}\")\n",
    "        # Fallback: write as pretty-printed string\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(all_responses, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Add responses to dataframe\n",
    "    df['gemini_response'] = response_texts\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Using the merged dictionary approach (all industries in one object)\n",
    "df_processed = process_dataframe_with_gemini(sp_analysis, 'dcf.json')\n",
    "\n",
    "\n",
    "df_processed.to_csv('sp_analysis_with_responses.csv', index=False)\n",
    "print(\"Processing complete. Check 'dcf.json' for properly formatted JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ac7e9",
   "metadata": {},
   "source": [
    "### Finally want to bring important Financial Accounts like total assets total liability revenue etc. Same logic as the DCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target accounts we're looking for\n",
    "target_accounts = {\n",
    "    'frame': 'frame',\n",
    "    'total_assets': 'Assets - TotalAssets',\n",
    "    'total_liabilities': 'Liabilities - TotalLiabilities', \n",
    "    'total_equity': 'Equity - TotalStockholdersEquity',\n",
    "    'total_revenue': 'IncomeStatement - Revenues - TotalRevenue',\n",
    "    'gross_profit': 'IncomeStatement - OtherIncomeExpense - GrossProfit',\n",
    "    'operating_income': 'IncomeStatement - OtherIncomeExpense - OperatingIncome',\n",
    "    'net_income': 'IncomeStatement - NetIncome - NetIncome',\n",
    "    'comprehensive_income': 'IncomeStatement - NetIncome - ComprehensiveIncome',\n",
    "    'basic_eps': 'IncomeStatement - EarningsPerShare - BasicEps',\n",
    "    'diluted_eps': 'IncomeStatement - EarningsPerShare - DilutedEps',\n",
    "    'shares_basic': 'IncomeStatement - EarningsPerShare - WeightedAverageSharesBasic',\n",
    "    'shares_diluted': 'IncomeStatement - EarningsPerShare - WeightedAverageSharesDiluted'\n",
    "}\n",
    "\n",
    "# Everytime I will manually change the Sector name to the one I want to analyze and save the data.\n",
    "sp_analysis = df_one_per_subindustry.copy()\n",
    "\n",
    "# Initialize counter and accounts storage\n",
    "ix = 0\n",
    "sp_analysis['accounts'] = None\n",
    "sp_analysis['question_prompt'] = None\n",
    "sp_analysis['available_target_accounts'] = None\n",
    "\n",
    "# Process each company\n",
    "for idx, row in sp_analysis.iterrows():\n",
    "    symbol = row['Symbol']\n",
    "    sub_industry = row['GICS Sub-Industry']\n",
    "    \n",
    "    # Retry configuration\n",
    "    max_retries = 3\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    \n",
    "    while retry_count < max_retries and not success:\n",
    "        try:\n",
    "            print(f'Processing {symbol} (Attempt {retry_count + 1}/{max_retries})')\n",
    "            \n",
    "            # Read the financial statement\n",
    "            df = pd.read_csv(f'./data/financial_statement/{symbol}_master.csv')\n",
    "            accounts = df.columns.to_list()\n",
    "            \n",
    "            # Store accounts in the dataframe\n",
    "            sp_analysis.at[idx, 'accounts'] = accounts\n",
    "            \n",
    "            # Find which target accounts are available (exact matches)\n",
    "            available_targets = {}\n",
    "            for key, target_col in target_accounts.items():\n",
    "                if target_col in accounts:\n",
    "                    available_targets[key] = target_col\n",
    "            \n",
    "            # Store available target accounts\n",
    "            sp_analysis.at[idx, 'available_target_accounts'] = available_targets\n",
    "            \n",
    "            # Create the prompt for mapping\n",
    "            accounts_str = ', '.join(accounts)\n",
    "            \n",
    "            # Convert sub_industry to PascalCase\n",
    "            pascal_sub_industry = ''.join(word.capitalize() for word in sub_industry.replace('-', ' ').replace('&', 'And').split())\n",
    "            \n",
    "            prompt = (\n",
    "                \"Analyze the financial statement columns and create a JSON mapping for these specific accounts. \"\n",
    "                \"Return ONLY the JSON mapping with no additional text or explanations.\\n\\n\"\n",
    "                f\"Industry: {sub_industry}\\n\"\n",
    "                f\"Available columns: {accounts_str}\\n\\n\"\n",
    "                \"Find exact matches or closest equivalents for these target accounts:\\n\"\n",
    "                \"1. frame - Time period identifier\\n\"\n",
    "                \"2. Assets - TotalAssets - Total assets from balance sheet\\n\"\n",
    "                \"3. Liabilities - TotalLiabilities - Total liabilities from balance sheet\\n\"\n",
    "                \"4. Equity - TotalStockholdersEquity - Total stockholders equity\\n\"\n",
    "                \"5. IncomeStatement - Revenues - TotalRevenue - Total revenue/sales\\n\"\n",
    "                \"6. IncomeStatement - OtherIncomeExpense - GrossProfit - Gross profit\\n\"\n",
    "                \"7. IncomeStatement - OtherIncomeExpense - OperatingIncome - Operating income\\n\"\n",
    "                \"8. IncomeStatement - NetIncome - NetIncome - Net income\\n\"\n",
    "                \"9. IncomeStatement - NetIncome - ComprehensiveIncome - Comprehensive income\\n\"\n",
    "                \"10. IncomeStatement - EarningsPerShare - BasicEps - Basic earnings per share\\n\"\n",
    "                \"11. IncomeStatement - EarningsPerShare - DilutedEps - Diluted earnings per share\\n\"\n",
    "                \"12. IncomeStatement - EarningsPerShare - WeightedAverageSharesBasic - Weighted avg shares basic\\n\"\n",
    "                \"13. IncomeStatement - EarningsPerShare - WeightedAverageSharesDiluted - Weighted avg shares diluted\\n\\n\"\n",
    "                f'Return the mapping in this exact JSON format:\\n'\n",
    "                '{\\n'\n",
    "                f'  \"{pascal_sub_industry}\": {{\\n'\n",
    "                '    \"frame\": [\"exact_column_name\"],\\n'\n",
    "                '    \"total_assets\": [\"exact_column_name\"],\\n'\n",
    "                '    \"total_liabilities\": [\"exact_column_name\"],\\n'\n",
    "                '    \"total_equity\": [\"exact_column_name\"],\\n'\n",
    "                '    \"total_revenue\": [\"exact_column_name\"],\\n'\n",
    "                '    \"gross_profit\": [\"exact_column_name\"],\\n'\n",
    "                '    \"operating_income\": [\"exact_column_name\"],\\n'\n",
    "                '    \"net_income\": [\"exact_column_name\"],\\n'\n",
    "                '    \"comprehensive_income\": [\"exact_column_name\"],\\n'\n",
    "                '    \"basic_eps\": [\"exact_column_name\"],\\n'\n",
    "                '    \"diluted_eps\": [\"exact_column_name\"],\\n'\n",
    "                '    \"shares_basic\": [\"exact_column_name\"],\\n'\n",
    "                '    \"shares_diluted\": [\"exact_column_name\"]\\n'\n",
    "                '  }\\n'\n",
    "                '}\\n\\n'\n",
    "                \"Rules: Use exact column names from the list above. If a column doesn't exist, use empty array []. No comments or extra text.\"\n",
    "            )\n",
    "            \n",
    "            sp_analysis.at[idx, 'question_prompt'] = str(prompt)\n",
    "            \n",
    "            # Mark as successful\n",
    "            success = True\n",
    "            print(f'Successfully processed {symbol}')\n",
    "            print(f'Available target accounts: {len(available_targets)}/{len(target_accounts)}')\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found for {symbol}. Skipping...')\n",
    "            # Don't retry for file not found errors\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f'Error processing {symbol}: {e}')\n",
    "            \n",
    "            if retry_count < max_retries:\n",
    "                print(f'Waiting 60 seconds before retry...')\n",
    "                time.sleep(60)  # Wait 1 minute before retrying\n",
    "            else:\n",
    "                print(f'Max retries reached for {symbol}. Moving to next company.')\n",
    "                # Optionally store error information\n",
    "                sp_analysis.at[idx, 'error'] = str(e)\n",
    "\n",
    "# Create a summary of what accounts are commonly available\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY OF ACCOUNT AVAILABILITY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze which target accounts are most commonly available\n",
    "account_availability = {}\n",
    "for key in target_accounts.keys():\n",
    "    account_availability[key] = 0\n",
    "\n",
    "for idx, row in sp_analysis.iterrows():\n",
    "    if row['available_target_accounts'] is not None:\n",
    "        for key in row['available_target_accounts'].keys():\n",
    "            account_availability[key] += 1\n",
    "\n",
    "total_companies = len(sp_analysis)\n",
    "print(f\"\\nTotal companies processed: {total_companies}\")\n",
    "print(\"\\nAccount availability across companies:\")\n",
    "for account, count in account_availability.items():\n",
    "    percentage = (count / total_companies) * 100\n",
    "    print(f\"  {account}: {count}/{total_companies} ({percentage:.1f}%)\")\n",
    "\n",
    "# Save results\n",
    "output_file = f'financial_mapping_analysis_{int(time.time())}.csv'\n",
    "sp_analysis.to_csv(output_file, index=False)\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "# Create a sample JSON structure for manual review\n",
    "sample_mapping = {}\n",
    "for idx, row in sp_analysis.iterrows():\n",
    "    if row['available_target_accounts'] is not None and len(row['available_target_accounts']) > 0:\n",
    "        symbol = row['Symbol']\n",
    "        sub_industry = row['GICS Sub-Industry']\n",
    "        pascal_sub_industry = ''.join(word.capitalize() for word in sub_industry.replace('-', ' ').replace('&', 'And').split())\n",
    "        \n",
    "        sample_mapping[symbol] = {\n",
    "            \"sub_industry\": sub_industry,\n",
    "            \"pascal_sub_industry\": pascal_sub_industry,\n",
    "            \"available_accounts\": row['available_target_accounts']\n",
    "        }\n",
    "\n",
    "# Save sample mapping for reference\n",
    "sample_file = f'sample_mapping_{int(time.time())}.json'\n",
    "with open(sample_file, 'w') as f:\n",
    "    json.dump(sample_mapping, f, indent=2)\n",
    "print(f\"Sample mapping saved to: {sample_file}\")\n",
    "\n",
    "print(f\"\\nProcessing complete! Review the prompts and use them with your AI to generate the final JSON mappings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ff737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extract JSON content from Gemini's response, handling markdown code blocks\n",
    "    \"\"\"\n",
    "    # Try to find JSON content within ```json ... ``` blocks\n",
    "    json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(json_pattern, response_text, re.DOTALL)\n",
    "    \n",
    "    if matches:\n",
    "        # Take the first match\n",
    "        json_str = matches[0].strip()\n",
    "    else:\n",
    "        # If no markdown blocks, try to find JSON content within { }\n",
    "        brace_pattern = r'\\{.*\\}'\n",
    "        brace_matches = re.findall(brace_pattern, response_text, re.DOTALL)\n",
    "        if brace_matches:\n",
    "            json_str = brace_matches[0].strip()\n",
    "        else:\n",
    "            # Assume the entire response is JSON\n",
    "            json_str = response_text.strip()\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON string\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"JSON string attempted: {json_str[:300]}...\")  # Show first 300 chars\n",
    "        return None\n",
    "\n",
    "def process_financial_mapping_with_gemini(df, output_file='financial_mappings.json'):\n",
    "    \"\"\"\n",
    "    Process each row's financial mapping prompt and save Gemini's response as proper JSON\n",
    "    \"\"\"\n",
    "    # Dictionary to store all financial mappings\n",
    "    all_mappings = {}\n",
    "    response_texts = []  # For dataframe column\n",
    "    parsed_mappings = []  # For dataframe column\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            symbol = row['Symbol']\n",
    "            prompt = row['question_prompt']\n",
    "            sub_industry = row['GICS Sub-Industry']\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing {index + 1}/{total_rows}: {symbol} - {sub_industry}\")\n",
    "            \n",
    "            # Send prompt to Gemini\n",
    "            response = model.generate_content(prompt)\n",
    "            response_text = response.text\n",
    "            \n",
    "            print('Response received from Gemini')\n",
    "            \n",
    "            # Parse JSON from response\n",
    "            parsed_json = parse_json_from_response(response_text)\n",
    "            \n",
    "            if parsed_json:\n",
    "                # If the parsed JSON has a single key (industry name),\n",
    "                # merge it into our main dictionary\n",
    "                if isinstance(parsed_json, dict) and len(parsed_json) == 1:\n",
    "                    all_mappings.update(parsed_json)\n",
    "                    # Get the key name for storage\n",
    "                    industry_key = list(parsed_json.keys())[0]\n",
    "                    parsed_mappings.append(parsed_json[industry_key])\n",
    "                else:\n",
    "                    # Otherwise, use the symbol as the key\n",
    "                    all_mappings[symbol] = parsed_json\n",
    "                    parsed_mappings.append(parsed_json)\n",
    "                \n",
    "                print(f\"✓ Successfully parsed JSON mapping for {symbol}\")\n",
    "                response_texts.append(json.dumps(parsed_json, indent=2))\n",
    "            else:\n",
    "                print(f\"✗ Failed to parse JSON for {symbol}\")\n",
    "                error_info = {\n",
    "                    \"error\": \"Failed to parse JSON\", \n",
    "                    \"raw_response\": response_text[:500],\n",
    "                    \"symbol\": symbol,\n",
    "                    \"sub_industry\": sub_industry\n",
    "                }\n",
    "                all_mappings[f\"{symbol}_ERROR\"] = error_info\n",
    "                response_texts.append(response_text)\n",
    "                parsed_mappings.append(error_info)\n",
    "            \n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            # Rate limiting - wait between requests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            print(f\"✗ Error processing {symbol}: {str(e)}\")\n",
    "            \n",
    "            # Store error in the mappings\n",
    "            error_info = {\n",
    "                \"error\": str(e),\n",
    "                \"symbol\": symbol,\n",
    "                \"sub_industry\": sub_industry,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            all_mappings[f\"{symbol}_ERROR\"] = error_info\n",
    "            response_texts.append(error_msg)\n",
    "            parsed_mappings.append(error_info)\n",
    "            \n",
    "            # Longer wait on error\n",
    "            time.sleep(10)\n",
    "    \n",
    "    # Write the combined JSON to file\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_mappings, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n✓ Financial mappings JSON file created: {output_file}\")\n",
    "        \n",
    "        # Also create a summary file\n",
    "        summary_file = output_file.replace('.json', '_summary.json')\n",
    "        summary = {\n",
    "            \"total_companies\": total_rows,\n",
    "            \"successful_mappings\": len([k for k in all_mappings.keys() if not k.endswith('_ERROR')]),\n",
    "            \"failed_mappings\": len([k for k in all_mappings.keys() if k.endswith('_ERROR')]),\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"industries_processed\": list(set(df['GICS Sub-Industry'].tolist()))\n",
    "        }\n",
    "        \n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✓ Summary file created: {summary_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error writing JSON file: {e}\")\n",
    "        # Fallback: write as pretty-printed string\n",
    "        fallback_file = output_file.replace('.json', '_fallback.txt')\n",
    "        with open(fallback_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(all_mappings, indent=2, ensure_ascii=False))\n",
    "        print(f\"✓ Fallback file created: {fallback_file}\")\n",
    "    \n",
    "    # Add responses to dataframe\n",
    "    df_copy = df.copy()\n",
    "    df_copy['gemini_response'] = response_texts\n",
    "    df_copy['parsed_mapping'] = parsed_mappings\n",
    "    \n",
    "    return df_copy, all_mappings\n",
    "\n",
    "def validate_financial_mappings(mappings_dict):\n",
    "    \"\"\"\n",
    "    Validate the financial mappings and provide a report\n",
    "    \"\"\"\n",
    "    validation_report = {\n",
    "        \"total_industries\": 0,\n",
    "        \"complete_mappings\": 0,\n",
    "        \"partial_mappings\": 0,\n",
    "        \"failed_mappings\": 0,\n",
    "        \"account_coverage\": {},\n",
    "        \"missing_accounts\": {}\n",
    "    }\n",
    "    \n",
    "    # Target accounts we're looking for\n",
    "    target_accounts = [\n",
    "        'frame', 'total_assets', 'total_liabilities', 'total_equity',\n",
    "        'total_revenue', 'gross_profit', 'operating_income', 'net_income',\n",
    "        'comprehensive_income', 'basic_eps', 'diluted_eps', 'shares_basic', 'shares_diluted'\n",
    "    ]\n",
    "    \n",
    "    for industry_key, mapping in mappings_dict.items():\n",
    "        if industry_key.endswith('_ERROR'):\n",
    "            validation_report[\"failed_mappings\"] += 1\n",
    "            continue\n",
    "            \n",
    "        validation_report[\"total_industries\"] += 1\n",
    "        \n",
    "        if isinstance(mapping, dict) and 'error' not in mapping:\n",
    "            # Count how many target accounts are mapped\n",
    "            mapped_accounts = []\n",
    "            missing_accounts = []\n",
    "            \n",
    "            for account in target_accounts:\n",
    "                if account in mapping and mapping[account] and len(mapping[account]) > 0:\n",
    "                    mapped_accounts.append(account)\n",
    "                else:\n",
    "                    missing_accounts.append(account)\n",
    "            \n",
    "            # Classify mapping completeness\n",
    "            coverage_ratio = len(mapped_accounts) / len(target_accounts)\n",
    "            if coverage_ratio >= 0.8:  # 80% or more\n",
    "                validation_report[\"complete_mappings\"] += 1\n",
    "            elif coverage_ratio >= 0.3:  # 30% or more\n",
    "                validation_report[\"partial_mappings\"] += 1\n",
    "            else:\n",
    "                validation_report[\"failed_mappings\"] += 1\n",
    "            \n",
    "            validation_report[\"account_coverage\"][industry_key] = {\n",
    "                \"mapped\": mapped_accounts,\n",
    "                \"missing\": missing_accounts,\n",
    "                \"coverage_ratio\": coverage_ratio\n",
    "            }\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "# Process the financial mapping data\n",
    "print(\"Starting financial mapping processing with Gemini...\")\n",
    "print(\"This may take several minutes depending on the number of companies...\")\n",
    "\n",
    "# Process the dataframe\n",
    "df_processed, financial_mappings = process_financial_mapping_with_gemini(\n",
    "    sp_analysis, \n",
    "    'financial_account_mappings.json'\n",
    ")\n",
    "\n",
    "# Validate the mappings\n",
    "validation_report = validate_financial_mappings(financial_mappings)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROCESSING COMPLETE - VALIDATION REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total industries processed: {validation_report['total_industries']}\")\n",
    "print(f\"Complete mappings (80%+ coverage): {validation_report['complete_mappings']}\")\n",
    "print(f\"Partial mappings (30-79% coverage): {validation_report['partial_mappings']}\")\n",
    "print(f\"Failed mappings (<30% coverage): {validation_report['failed_mappings']}\")\n",
    "\n",
    "# Save the processed dataframe\n",
    "df_processed.to_csv('financial_mapping_results.csv', index=False)\n",
    "print(f\"\\n✓ Processed dataframe saved to: financial_mapping_results.csv\")\n",
    "\n",
    "# Save validation report\n",
    "with open('mapping_validation_report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validation_report, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Validation report saved to: mapping_validation_report.json\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FILES CREATED:\")\n",
    "print(\"1. financial_account_mappings.json - Main mapping file\")\n",
    "print(\"2. financial_account_mappings_summary.json - Processing summary\")\n",
    "print(\"3. financial_mapping_results.csv - Dataframe with responses\")\n",
    "print(\"4. mapping_validation_report.json - Validation analysis\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nProcessing complete! Review the JSON files for your financial account mappings.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
