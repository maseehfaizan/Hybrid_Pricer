{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Data Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_reddit_client():\n",
    "    \"\"\"Initialize and return a Reddit client using PRAW.\"\"\"\n",
    "    load_dotenv(\"api.env\")\n",
    "    client_id = os.getenv('REDDIT_CLIENT_ID')\n",
    "    client_secret = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "    user_agent = os.getenv('REDDIT_USER_AGENT', 'StockDataScraper v1.0')\n",
    "    \n",
    "    if not client_id or not client_secret:\n",
    "        raise ValueError(\"Reddit API credentials not found in api.env file\")\n",
    "    \n",
    "    return praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get S&P 500 Tickers from Wikipedia\n",
    "URL = https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_tickers():\n",
    "    \"\"\"Get S&P 500 tickers from Wikipedia.\"\"\"\n",
    "    try:\n",
    "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "        sp500 = pd.read_html(str(table))[0]\n",
    "        return sp500['Symbol'].to_list()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching S&P 500 tickers: {e}\")\n",
    "        return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA']  # Fallback tickers M7s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_data(reddit, subreddit_name, data_type='posts', search_term=None, \n",
    "                   time_filter='year', limit=100, comment_limit=25):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_list = []\n",
    "    comments_list = []\n",
    "    \n",
    "    try:\n",
    "        # Determine which data to fetch\n",
    "        if data_type == 'search' and search_term:\n",
    "            print(f\"Searching for '{search_term}' in r/{subreddit_name}...\")\n",
    "            posts = subreddit.search(search_term, limit=limit)\n",
    "            search_keywords = [search_term.lower(), f\"${search_term.lower()}\"]\n",
    "        else:\n",
    "            print(f\"Getting top posts from r/{subreddit_name} for {time_filter}...\")\n",
    "            posts = subreddit.top(time_filter=time_filter, limit=limit)\n",
    "            search_keywords = None\n",
    "            \n",
    "        # Process posts\n",
    "        for i, post in enumerate(posts):\n",
    "            # Filter search results if needed\n",
    "            if search_keywords and not any(kw in (post.title + \" \" + post.selftext).lower() for kw in search_keywords):\n",
    "                continue\n",
    "                \n",
    "            # Extract post data\n",
    "            post_data = {\n",
    "                'post_id': post.id,\n",
    "                'title': post.title,\n",
    "                'selftext': post.selftext,\n",
    "                'score': post.score,\n",
    "                'upvote_ratio': post.upvote_ratio,\n",
    "                'created_utc': datetime.datetime.fromtimestamp(post.created_utc),\n",
    "                'num_comments': post.num_comments,\n",
    "                'author': str(post.author),\n",
    "                'permalink': post.permalink,\n",
    "                'url': post.url,\n",
    "                'is_self': post.is_self,\n",
    "                'flair': post.link_flair_text,\n",
    "                'subreddit': subreddit_name,\n",
    "                'category': 'stock_specific' if data_type == 'search' else 'general'\n",
    "            }\n",
    "            \n",
    "            # Add search term if applicable\n",
    "            if search_term:\n",
    "                post_data['search_term'] = search_term\n",
    "                \n",
    "            posts_list.append(post_data)\n",
    "            \n",
    "            # Get comments\n",
    "            try:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                for comment in post.comments.list()[:comment_limit]:\n",
    "                    # Filter comments for search terms if needed\n",
    "                    if search_keywords and not any(kw in comment.body.lower() for kw in search_keywords):\n",
    "                        continue\n",
    "                        \n",
    "                    comment_data = {\n",
    "                        'comment_id': comment.id,\n",
    "                        'post_id': post.id,\n",
    "                        'parent_id': comment.parent_id,\n",
    "                        'body': comment.body,\n",
    "                        'score': comment.score,\n",
    "                        'created_utc': datetime.datetime.fromtimestamp(comment.created_utc),\n",
    "                        'author': str(comment.author),\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'category': 'stock_specific' if data_type == 'search' else 'general'\n",
    "                    }\n",
    "                    \n",
    "                    # Add search term if applicable\n",
    "                    if search_term:\n",
    "                        comment_data['search_term'] = search_term\n",
    "                        \n",
    "                    comments_list.append(comment_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing comments for post {post.id}: {e}\")\n",
    "                \n",
    "            # Be nice to Reddit's servers\n",
    "            if (i + 1) % 25 == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "        print(f\"Found {len(posts_list)} posts and {len(comments_list)} comments\")\n",
    "        return pd.DataFrame(posts_list) if posts_list else pd.DataFrame(), \\\n",
    "               pd.DataFrame(comments_list) if comments_list else pd.DataFrame()\n",
    "               \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from r/{subreddit_name}: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Data Collection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create timestamp and directory\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    data_dir = f'reddit_data_{timestamp}'\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Initialize Reddit client\n",
    "        reddit = initialize_reddit_client()\n",
    "        \n",
    "        # Get S&P 500 tickers\n",
    "        sp500_tickers = get_sp500_tickers()\n",
    "        \n",
    "        # List of finance subreddits\n",
    "        subreddits = ['wallstreetbets', 'stocks', 'investing', 'StockMarket']\n",
    "        \n",
    "        # Initialize aggregated DataFrames\n",
    "        all_posts = pd.DataFrame()\n",
    "        all_comments = pd.DataFrame()\n",
    "        \n",
    "        # 1. Get general top posts from each subreddit\n",
    "        for subreddit in subreddits:\n",
    "            posts_df, comments_df = get_reddit_data(\n",
    "                reddit, subreddit, data_type='posts', time_filter='year', \n",
    "                limit=100, comment_limit=25\n",
    "            )\n",
    "            \n",
    "            all_posts = pd.concat([all_posts, posts_df], ignore_index=True)\n",
    "            all_comments = pd.concat([all_comments, comments_df], ignore_index=True)\n",
    "            \n",
    "            # Save individual subreddit data\n",
    "            subreddit_dir = os.path.join(data_dir, subreddit)\n",
    "            os.makedirs(subreddit_dir, exist_ok=True)\n",
    "            \n",
    "            if not posts_df.empty:\n",
    "                posts_df.to_csv(f'{subreddit_dir}/{subreddit}_general_posts.csv', index=False)\n",
    "            if not comments_df.empty:\n",
    "                comments_df.to_csv(f'{subreddit_dir}/{subreddit}_general_comments.csv', index=False)\n",
    "                \n",
    "            time.sleep(2)  # Be nice to Reddit's servers\n",
    "        \n",
    "        # 2. Get stock-specific posts (limit to first 30 tickers for efficiency)\n",
    "        for subreddit in subreddits:\n",
    "            for ticker in sp500_tickers[:30]:  # Limit to prevent excessive API calls\n",
    "                posts_df, comments_df = get_reddit_data(\n",
    "                    reddit, subreddit, data_type='search', search_term=ticker,\n",
    "                    limit=50, comment_limit=25\n",
    "                )\n",
    "                \n",
    "                all_posts = pd.concat([all_posts, posts_df], ignore_index=True)\n",
    "                all_comments = pd.concat([all_comments, comments_df], ignore_index=True)\n",
    "                \n",
    "                # Save ticker data to subreddit directory\n",
    "                subreddit_dir = os.path.join(data_dir, subreddit)\n",
    "                if not posts_df.empty:\n",
    "                    ticker_file = f'{subreddit_dir}/{subreddit}_{ticker}_posts.csv'\n",
    "                    posts_df.to_csv(ticker_file, index=False)\n",
    "                if not comments_df.empty:\n",
    "                    ticker_file = f'{subreddit_dir}/{subreddit}_{ticker}_comments.csv'\n",
    "                    comments_df.to_csv(ticker_file, index=False)\n",
    "                    \n",
    "                time.sleep(1)  # Be nice to Reddit's servers\n",
    "            \n",
    "            time.sleep(2)  # Additional pause between subreddits\n",
    "            \n",
    "        # Save aggregated data\n",
    "        all_posts.to_csv(f'{data_dir}/all_posts.csv', index=False)\n",
    "        all_comments.to_csv(f'{data_dir}/all_comments.csv', index=False)\n",
    "        \n",
    "        # Create summary\n",
    "        summary = {\n",
    "            'total_posts': len(all_posts),\n",
    "            'total_comments': len(all_comments),\n",
    "            'subreddits': subreddits,\n",
    "            'tickers_searched': sp500_tickers[:30],\n",
    "            'time_filter': 'year',\n",
    "            'collection_date': datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "        }\n",
    "        \n",
    "        pd.Series(summary).to_json(f'{data_dir}/collection_summary.json')\n",
    "        \n",
    "        print(f\"\\nData collection complete!\")\n",
    "        print(f\"Total posts: {len(all_posts)}\")\n",
    "        print(f\"Total comments: {len(all_comments)}\")\n",
    "        print(f\"Data saved in: {data_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
